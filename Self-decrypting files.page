# Delayed self-decrypting files

Julian Assange/Wikileaks made some [headlines](http://www.wired.com/threatlevel/2010/07/wikileaks-insurance-file/) in 2010 when they released an 'insurance file', an 1.4GB [AES-256](!Wikipedia)-encrypted file available through BitTorrent. It's generally assumed that copies of the encryption key have been left with Wikileaks supporters who will, in the appropriate contingency like Assange being assassinated, leak the key online to the thousands of downloaders of the insurance file, who will then read and publicize whatever contents as in it (speculated to be additional US documents Manning gave Wikileaks).

Of course, any one of those supporters could become disaffected and leak the key at any time. Or if there's only 1 supporter, they might lose the key to a glitch or become disaffected in the opposite direction and refuse to transmit the key to anyone. If one trusts the person with the key *absolutely*, that's fine. But wouldn't it be nice if one didn't have to trust another person like that? Cryptography does really well at eliminating the need to trust others, so maybe there're better schemes.

Now, it's hard to imagine how some abstract math could observe an assassination and decrypt embarrassing files. Perhaps a different question could be answered - can you design an encryption scheme which requires no trusted parties but can be broken after a certain date?

## No trusted third-parties

Note that this bars a lot of the [usual suggestions](http://www.halfbakery.com/idea/Do_20not_20decrypt_20until_20_2e_2e_2e) or cryptography schemes. For example, if you trust some people, you can just adopt a [secret sharing](!Wikipedia) protocol where they XOR together their keys to get the master key for the publicly distributed encrypted file. Or if you only trust some of those people (but are unsure which will try to betray you and either release early or late), you can adopt where _k_ of the _n_ people suffice to reconstruct the master key. (And you can connect multiple groups, so each decrypts some necessary keys for the next group; but this gives each group a consecutive veto on release...) Or perhaps something could be devised based on [trusted timestamping](!Wikipedia) (but then don't you need the [trusted third party](!Wikipedia) to at least survive on the network?) or [secure multi-party computation](!Wikipedia) (but don't *you* need to be on the network, or risk all the parties saying 'screw it, we're too impatient, let's just pool our secrets and decrypt the file *now*'?) or...

One approach is to focus on creating problems which can be solved with a large but precise amount of work, reasoning that if the problem can't be solved in less than a month, then you can use that as a way to guarantee the file can't be decrypted *within* a month's time. (This would be a [proof-of-work system](!Wikipedia).)

## Hashing

For example, one could take a [hash](!Wikipedia "Cryptographic hash function") like [bcrypt](!Wikipedia), give it a random input, and hash it for a month. Each hash depends on the previous hash, and there's no way to skip from the first hash to the trillionth hash. After a month, you use the final hash as the encryption key, and then release the encrypted file and the random input to all the world. The first person who wants to decrypt the file has no choice but to redo the trillion hashes in order to get the same encryption key you used.

Nor can the general public (or the NSA) exploit the parallelism they have available, because each hash depends sensitively on the hash before it - the [avalanche effect](!Wikipedia) is a key property to cryptographic hashes. On the other hand, the person running this algorithm can run it in parallel.

One generates _n_ random inputs (for _n_ CPUs, presumably), and sets them hashing as before for however long one can spare. Then, one sets up a chain between the _n_ results - the final hash of seed 1 is used to encrypt seed 2, the final hash of which was the encryption for seed 3, and so on. Then one releases the encrypted file, the $n-1$ encrypted seeds, and the first seed. Now the public has to hash the first seed for a month, and only then can it unlock the second seed, and start hashing *that* for a month, and so on.

This is pretty clever. If one has a thousand CPUs handy, one can store up 3 years' of computation-resistance in just a day. This satisfies a number of needs. But what about people who only have a normal computer? Fundamentally, this repeated hashing requires you to put in as much computation as you want your public to expend reproducing the computation, which is not enough. We want to force the public to expend more computation - potentially much more - than we put in. How can we do this?

It's hard to see. At least, I haven't thought of anything clever. [Homomorphic encryption](!Wikipedia) promises to let us encode arbitrary computations into an encrypted file, so one could imagine implementing the above hash chains *inside* the homomorphic computation, or perhaps just encoding a simple loop counting up to a very large number; but it's not clear how one would let the public decrypt the result of the homomorphic encryption without also letting them tamper with the loop or whatever, and in any case, homomorphic encryption is currently a net-loss - it takes as much or more CPU time to create such a program as it would take to run the program, and in that case, one might as well use the previous hash schemes.

## Successive squaring

At this point, let's see what the crypto experts have to say. Googling to see what the existing literature was (after I'd thought of the above schemes), I found that the relevant term is "time-lock puzzles" (from analogy with the bank vault [time lock](!Wikipedia)). In particular, [Rivest](!Wikipedia "Ron Rivest")/[Shamir](!Wikipedia "Adi Shamir")/[Wagner](!Wikipedia "David A. Wagner") have published a 1996 paper on the topic, ["Time-lock puzzles and timed-release crypto"](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.110.5709). The very first paragraph is interesting; apparently the question was first raised by [Timothy C. May](!Wikipedia) on the [Cypherpunks mailing list](!Wikipedia). Unfortunately, there seem to be no archives online of the cypherpunk mailing list (the provided URL is long dead); fortunately, May discusses the topic briefly in his _[Cyphernomicon](!Wikipedia)_, [ch14.5](http://www.cypherpunks.to/faq/cyphernomicron/chapter14.html); unfortunately, May's solution (14.5.1) is essentially to punt to the legal system and rely on legal privilege and economic incentives to keep keys private.

Rivest et al agree with us that

> "There are 2 natural approaches to implementing timed-release crypto:
>
> - Use 'time-lock puzzles' - computational problems that can not be solved without running a computer continuously for at least a certain amount of time.
> - Use trusted agents who promise not to reveal certain information until a specified date."

And that for time-lock puzzles:

> "Our goal is thus to design time-lock puzzles that, to the great extent possible, are 'intrinsically sequential' in nature, and can not be solved substantially faster with large investments in hardware. In particular, we want our puzzles to have the property that putting computers to work together in parallel doesn't speed up finding the solution. (Solving the puzzle should be like having a baby: two women can't have a baby in 4.5 months.)"

Rivest et al then points out that the most obvious approach - encrypt the file to a random short key, short enough that brute-forcing takes only a few months/years as opposed to eons - is flawed because brute-forcing a key is very [parallelizable](!Wikipedia "EFF DES cracker"). (And as well, the randomness of searching a key space means that the key might be found very early or very late; any estimate of how long it will take to brute force is just a guess.)

Rivest et al propose a scheme in which one encrypts the file with a very strong key as usual, but then one encrypts the key in such a way that one must calculate $\text{encryptedKey}^{2^t} \text{mod}(n)$ where _t_ is the adjustable difficulty factor. With the original numbers, one can easily avoid doing the [successive squarings](http://mathworld.wolfram.com/SuccessiveSquareMethod.html) since it's just $2^t = e \times \text{mod}(\Phi(n))$ (Rivest says).

### Constant factors

How well does this work? The complexity seems correct, but I worry about the constant factors. Back in 1996, computers were fairly homogeneous, and Rivest et al could reasonably write

> "We know of no obvious way to parallelize it to any large degree. (A small amount of parallelization may be possible *within* each squaring.) The degree of variation in how long it might take to solve the puzzle depends on the variation in the speed of single computers and not on one's total budget. Since the speed of hardware available to individual consumers is within a small constant factor of what is available to large intelligence organizations, the difference in time to solution is reasonably controllable."

But that doesn't seem very true any more. Devices can differ dramatically now even in the same computers; to take the example of [Bitcoin](!Wikipedia) mining, my laptop's CPU can search for hashes at 4k/sec, or its GPU can search at 45*m*/second^[Actual numbers; the difference [really is](https://en.bitcoin.it/wiki/Why_a_GPU_mines_faster_than_a_CPU) that [large](https://en.bitcoin.it/wiki/Mining_hardware_comparison).]. Many [scientific applications](!Wikipedia "GPGPU#Applications") have moved to clusters of GPUs because they offer such great speedups; as have a number of cryptographic applications such as generating[^rainbow] [rainbow tables](!Wikipedia).

[^rainbow]: eg. the 2008 Graves thesis, ["High performance password cracking by implementing rainbow tables on nVidia graphics cards (IseCrack)"](https://www.iac.iastate.edu/mediawiki/images/b/b3/Cryptohaze.pdf) claims a 100x speedup over CPU generation of rainbow tables, or the actively developed utility, [RainbowCrack](http://www.project-rainbowcrack.com/) (which you can even [buy](http://www.project-rainbowcrack.com/buy.php) the generated rainbow tables from).

And then there are more exotic technologies like [field-programmable gate array](!Wikipedia)s which may be specialized for successive squaring; if problems like the [n-body problem](!Wikipedia) can be [handled with custom chips](!Wikipedia "Gravity Pipe"), why not multiplication? Offhand, I don't know of any compelling argument to the effect that there are no large constant-factor speedups possible for multiplication/successive-squaring.

## Memory-bound hashes

Of course, one could ask the same question of my original proposal - what makes you think that hashing can't be sped up? You already supplied an example where cryptographic hashes were sped up astonishingly by a GPU, Bitcoin mining.

The difference is that hashing can be made to stress the weakest part of any modern computer system, the [memory hierarchy](!Wikipedia)'s terrible bandwidth and latency[^latency]; the hash can blow the fast die-level caches (the [CPU](!Wikipedia "Processor register") & its [cache](!Wikipedia "CPU cache")) and force constant fetches from the main RAM. They were devised for anti-spam proof-of-work systems that wouldn't unfairly penalize cellphones & PDAs while still being costly on desktops & workstations (which rules out the usual functions like [Hashcash](!Wikipedia) that stress the CPU). For example, the 2003 ["On Memory-Bound Functions for Fighting Spam"](http://research.microsoft.com/apps/pubs/?id=65154) ([PDF](http://research.microsoft.com/research/sv/sv-pubs/crypto03.pdf)); from the abstract:

> "Burrows suggested that, since memory access speeds vary across machines much less than do CPU speeds, memory-bound functions may behave more equitably than CPU-bound functions; this approach was first explored by Abadi, Burrows, Manasse, and Wobber [8]. We further investigate this intriguing proposal. Specifically, we...
>
> 2. Provide an abstract function and prove an asymptotically tight amortized lower bound on the number of memory accesses required to compute an acceptable proof of effort; specifically, we prove that, on average, the sender of a message must perform many unrelated accesses to memory, while the receiver, in order to verify the work, has to perform significantly fewer accesses;
> 3. Propose a concrete instantiation of our abstract function, inspired by the RC4 stream cipher;
> 4. Describe techniques to permit the receiver to verify the computation with no memory accesses; 5. Give experimental results showing that our concrete memory-bound function is only about four times slower on a 233 MHz settop box than on a 3.06 GHz workstation, and that speedup of the function is limited even if an adversary knows the access sequence and uses optimal off-line cache replacement."

Abadi 2005, ["Moderately hard, memory-bound functions"](http://portal.acm.org/citation.cfm?id=1064341) ([PDF](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.6879&rep=rep1&type=pdf)) develop more memory-bound functions and benchmark them:

> "...we give experimental results for five modern machines that were bought within a two-year period in 2000-2002, and which cover a range of performance characteristics. All of these machines are sometimes used to send e-mail-even the settop box,which is employed as a quiet machine in a home....None of the machines have huge caches-the largest was on the server machine, which has a 512KB cache. Although the clock speeds of the machines vary by a factor of 12, the memory read times vary by a factor of only 4.2. This measurement confirms our premise that memory read latencies vary much less than CPU speeds.
>
> ...At the high end, the server has lower performance than one might expect, because of a complex pipeline that penalizes branching code. In general, higher clock speeds correlate with higher performance, but the correlation is far from perfect...Second, the desktop machine is the most cost-effective one for both CPU-bound and memory-bound computations; in both cases, attackers are best served by buying the same type of machines as ordinary users. Finally, the memory-bound functions succeed in maintaining a performance ratio between the slowest and fastest machines that is not much greater than the ratio of memory read times."

[^latency]: From Abadi 2005:

    > "Fast CPUs run much faster than slow CPUs-consider a 2.5GHz PC versus a 33MHz Palm PDA. Moreover, in addition to high clock rates, higher-end computer systems also have sophisticated pipelines and other advantageous features. If a computation takes a few seconds on a new PC, it may take a minute on an old PC, and several minutes on a PDA. That seems unfortunate for users of old PCs, and probably unacceptable for users of PDAs....we are concerned with finding moderately hard functions that most computer systems will evaluate at about the same speed. We envision that high-end systems might evaluate these functions somewhat faster than low-end systems, perhaps even 2-10 times faster (but not 10-100 faster, as CPU disparities might imply). Moreover, the best achievable price-performance should not be significantly better than that of a typical legitimate client....A memory-bound function is one whose computation time is dominated by the time spent accessing memory. The ratios of memory latencies of machines built in the last five years is typically no greater than two, and almost always less than four. (Memory throughput tends to be less uniform, so we focus on latency.) A memory-bound function should access locations in a large region of memory in an unpredictable way, in such a way that caches are ineffectual....Other possible applications include establishing shared secrets over insecure channels and the timed release of information, using memory-bound variants of Merkle puzzles [Merkle 1978] and of time-lock puzzles [May 1993; Rivest et al. 1996], respectively. We discuss these also in section 4."

    And here I thought I was being original in suggesting memory-bound functions for time-lock puzzles! Truly, 'there is nothing new under the sun'.
