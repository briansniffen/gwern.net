---
description: Misc. thoughts, memories, proto-essays, musings, etc.
...

"And on that dread day, the Ineffable One will summon the artificers and makers of graven images, and He will command them to give life to their creations, and failing, they and their creations will be dedicated to the flames..."

---

"Some say that a god lives on in the faith and memory of its believers. They point to computers and say, 'Behold, they need but think all together in a particular & precise mode, and from nowhere appear things real and greater than any they thought. Might not the same be true of humans, who are so much greater?' But this is no more true than a painting of a flower the flower itself."

# _Evangelion_'s influence on _RahXephon_

- Paper idea: ''The anxiety of influence: RahXephon's response to Neon Genesis Evangelion''
        - [The Anxiety of Influence](!Wikipedia). every artist makes his predecessors... Borges. RahXephon and Eva... [RahXephon#Neon Genesis Evangelion](!Wikipedia) (comparison). Eva ineluctably influenced RahXephon... RahXephon's manga begun 2001, Evangelion's TV 1995. mecha anime are remixes... variations. the pleasure of watching one is seeing the variation on the Truth, of trying to see each one get closer and closer to the heart of the matter. "I've taken on a risk: "It's just an imitation". And for now I can only write this explanation. But perhaps our "original" lies somewhere within there." (Hideaki Anno, from his story treatment "What were we trying to make here?" written before NGE began being produced by Gainax, as recorded on page 171 of ''Neon Genesis Evangelion Volume 1'', Yoshiyuki Sadamoto, translated by Fred Burke. August 2003. ISBN 1-56931-294-X).
          There is a deep relation here to Japanese poetry, in which originality is not necessarily valued. every change in a mecha anime from its predecessors is a reply, an ongoing dialog back and forth. how do mecha change? where have they gone to? look for the RahXephon bibles. Evangelion used Greek in Evangelion... rahx-ephon - "-ephon" as a suffix for instrument". RahXephon's creators wanted to create something new... track down references 2-5 of [RahXephon#Notes and reference](!Wikipedia). See also

# Long term investment

> "That is, from January 1926 through December 2002, when holding periods were 19 years or longer, the cumulative real return on stocks was never negative..."

How does one engage in extremely long investments? On a time-scale of centuries, investment is a difficult task, especially if one seeks to avoid erosion of returns by the costs of active management.

'Unit Investment Trust (UIT) is a US investment company offering a fixed (unmanaged) portfolio of securities having a definite life.'

'A closed-end fund is a collective investment scheme with a limited number of shares'

In long-term investments, one must become concerned about biases in the data used to make decisions. Many of these biases fall under the general rubric of "observer biases" - the canonical example being that stocks look like excellent investments if you only consider America's stock market, where returns over long periods have been quite good. For example, if you had invested by tracking the major indices any time period from January 1926 through December 2002 and had held onto your investment for at least 19 years, you were guaranteed a positive real return. Of course, the specification of place (America) and time period (before the Depression and after the Internet bubble) should alert us that this guarantee may not hold elsewhere. Had a long-term investor in the middle of the 19th century decided to invest in a large up-and-coming country with a booming economy and strong military (much like the United States has been for much of the 20th century), they would have reaped excellent returns. That is, until the hyperinflation of the Wiemar Republic. Should their returns have survived the inflation and imposition of a new currency, then the destruction of the 3rd Reich would surely have rendered their shares and Reichmarks worthless. Similarly for another up-and-coming nation - Japan. Mention of Russia need not even be made.

Clearly, diversifying among companies in a sector, or even sectors in a national economy is not enough. Disaster can strike an entire nation. Rosy returns for stocks quietly ignore those bloody years in which exchanges plunged thousands of percent in real terms, and whose records burned in the flames of war. Over a timespan of a century, it is impossible to know whether such destruction will be visited on a given country or even whether it will still exist as a unit. How could Germany, the preeminent power on the Continent, with a burgeoning navy rivaling Britain's, with the famous Prussian military and Junkers, with an effective industrial economy still famed for the quality of its mechanisms, and with a large homogeneous population of hardy people possibly fall so low as to be utterly conquered? And by the United States and others, for that matter? How could Japan, with its fanatical warriors and equally fanatical populace, its massive fleet and some of the best airplanes in the world - a combination that had humbled Russia, that had occupied Korea for nigh on 40 years, which easily set up puppet governments in Manchuria and China when and where it pleased - how could it have been defeated so wretchedly as to see its population literally decimated and its governance wholly supplanted? How could a god be dethroned?

It is perhaps not too much to say that investors in the United States, who say that the Treasury Bond has never failed to be redeemed and that the United States can never fall, are perhaps overconfident in their assessment. Inflation need not be hyper to cause losses. Greater nations have been destroyed quickly. Who remembers the days when the Dutch fought the English and the French to a standstill and ruled over the shipping lanes? Remember that Nineveh is one with the dust.

In short, our data on returns is biased. This bias indicates that stocks and cash are much more risky than most people think, and that this risk inheres in exogenous shocks to economies - it may seem odd to invest globally, in multiple currencies, just to avoid the rare black swans of total war and hyperinflation. But these risks are catastrophic risks. Even one may be too many.

This risk is more general. Governments can die, and so their bonds and other instruments (such as cash) rendered worthless; how many governments have died or defaulted over the last century? Many. The default assumption must be that the governments with good credit, who are not in that number, may simply have been lucky. And luck runs out.

In general, entities die unpredictably, and one has no guarantee that a, say, 1500 year old Korean construction company will honor its bills in another 500 years because all it takes is one bubble to drive it into bankruptcy. When one looks at securities turning into money, of course all you see are ones for those entities which survived. This is 'survivorship bias'; our observations are biased because we aren't looking at all of the past, but the present. This can be exploited, however. Obviously if an entity perishes, it has no need for assets.

Suppose one wishes to make a very long-term investment. One groups with a large number of other investors who wish to make similar investments, in a closed-end mutual fund with a share per investor, which is set to liquidate at some remote period. This fund would invest in assets all over the world and of various kinds, seeking great diversification. The key ingredient would be that shares are not allowed to be transferred. Should an investor perish, the value of their share would be split up amongst the other investors' shares (a percentage could be used to pay for management, perhaps). Because of this ingredient, the expected return for any individual investor would be extremely high - the potential loss is 100%, but the investor by definition will never be around for that loss. Because the identity and number of investments is fixed, potential control of the assets could be dispersed among the investors so as to avoid the situation where war destroys the headquarters of whomever is managing the assets. The technical details are unimportant; cryptography has many ingenious schemes for such matters (one can easily heavily encrypt a file and then distribute n keys where n - k are needed to decrypt the file).

'Suppose you think that gold will become worthless on April 27th, 2020 at between four and four-thirty in the morning. I, on the other hand, think this event will not occur until 2030. We can sign a contract in which I pay you one ounce of gold per year from 2010 to 2020, and then you pay me two ounces of gold per year from 2020 to 2030. If gold becomes worthless when you say, you will have profited; if gold becomes worthless when I say, I will have profited. We can have a prediction market on a generic apocalypse, in which participants who believe in an earlier apocalypse are paid by believers in a later apocalypse, until they pass the date of their prediction, at which time the flow reverses with interest. I don't see any way to distinguish between apocalypses, but we can ask the participants why they were willing to bet, and probably receive a decent answer.'

# American light novels

    01:04 < gwern> I think one of the more interesting trends in anime is the massive number of adaptations of light novels done in the '90s and 00s; it is interesting because no such trend exists in American media as far as I can tell (the closest I can think of are comic book adaptations, but of course those are analogous to the many mangas -> animes)
    01:04 < gwern> now, American media absolutely adapts many novels, but they are all normal Serious Business Novels
    01:05 < gwern> we do not seem to even have the light novel media - young adult novels do not cut the mustard. light novels are odd as they are kind of like speculative fiction novellas

question for self: if America doesn't have the light novel category, is that a claim that the _Twilight_ novels, and everything published under the James Patterson brand, are regular novels?

answer: The _Twilight_ novels are no more light novels than the _Harry Potter_ novels were. The Patterson novels may fit, however; they have some of the traits such as very short chapters, simple literary style, and very quick moving plots, even though they lack a few less important traits (such as including illustrations). It might be better to say that there is no recognized and successful light novel *genre* rather than individual light novels - there are only unusual examples like the Patterson novels and other works uncomfortable listed under the Young Adult/Teenager rubric.

# Cultural growth through diversity

In _[Human Accomplishment](!Wikipedia)_, one of the strongest indicators for genius is contact with a foreign culture. However, the modern era which is likely the most globalized one, in which English-speakers have access like they have never had before (compare how much classic Japanese & Chinese literature has been translated and stored in libraries as of 2009 to what was available when Waley began translating _Genji Monogatari_ in 1921!). This would seem to be something of a contradiction: if a little foreign contact was enough to inspire the 'chinoiserie' of France's Rococo period, then why wouldn't all the Asian immigrants and translations and economic contact to America spark even greater revolutions? There has been influence, absolutely; but the influence is striking for how a little bit helped (how many haiku did the Imagists have access to?) and a lot done not much more, and perhaps even less.

This is a little perplexing. What's the explanation? Did the early artists suck out all the novelty? Or is there something stimulating about having only a few examples - does one draw faulty but fruitful inferences based on idiosyncrasies of the small data set?

# Newcomb voting

What's the connection between Newcomb's Paradox and large-scale voting?


# Decluttering

[Ego depletion](!Wikipedia):

> 'Ego depletion refers to the idea that self-control and other mental processes that require focused conscious effort rely on energy that can be used up. When that energy is low (rather than high), mental activity that requires self-control is impaired. In other words, using one's self-control impairs the ability to control one's self later on. In this sense, the idea of (limited) willpower is correct.'

Wonder whether this has any connection with minimalism?
Clutter might just damage [Executive functions](!Wikipedia)

See also [latent inhibition](!Wikipedia)

> "Latent inhibition is a process by which exposure to a stimulus of little or no consequence prevents conditioned associations with that stimulus being formed. The ability to disregard or even inhibit formation of memory, by preventing associative learning of observed stimuli, is an automatic response and is thought to prevent information overload. Latent inhibition is observed in many species, and is believed to be an integral part of the observation/learning process, to allow the self to interact successfully in a social environment."

> "Most people are able to shut out the constant stream of incoming stimuli, but those with low latent inhibition cannot. It is hypothesized that a low level of latent inhibition can cause either psychosis, a high level of creativity[1] or both, which is usually dependent on the subject's intelligence.[2][3] Those of above average intelligence are thought to be capable of processing this stream effectively, an ability that greatly aids their creativity and ability to recall trivial events in incredible detail and which categorizes them as almost creative geniuses. Those with less than average intelligence, on the other hand, are less able to cope, and so as a result are more likely to suffer from mental illness."


Interesting decluttering approach: 100 Things Challenge

- <http://zenhabits.net/2007/09/minimalist-fun-the-100-things-challenge/>
- <http://www.guynameddave.com/100-thing-challenge.html>
- <http://www.time.com/time/magazine/article/0,9171,1812048,00.html>
- <http://www.denverpost.com/room/ci_8060057>

# _The Count of Zarathustra_

The count of Monte Cristo as a Nietzschean hero?

# Milgram authority experiments

Professor Grim tells a story about an acquaintance of his, who had once accepted a job at Yale's psychology department. He was driving into New Haven on his way to it, when he stopped for gas. The fellow working the station began filling up his car and they fell to chatting. (This was long ago, when such things still happened.) When he mentioned that he was going to his new job there, the attendant stiffened up, removing the gas pump, and retreated into the far side of the garage. Incensed, he followed the attendant and demanded an answer. The attendant eventually stopped the cold shoulder and quietly said that once he had participated in Milgram's infamous authoritarian experiments, and was one of those who had gone through with it all. "And", he added, "I have not slept well since."

(Grim added in clarification that the entire psychology department was in opprobrium with the New Haven population.)

# Title

Good poem title: 'The Scarecrow Appeals to Glenda the Good'

# Idea for Twitter SF

novel idea: an ancient British family has a 144 character (no spaces) string which encodes the political outcomes of the future eg. the restoration, the Glorious Rebellion, Napoleon, Nazis etc. thus the family has been able to pick the winning side every time and maintain its place. but they cannot interpret the remaining characters pertaining to our time. they hire researcher/librarians to crack it. one of them is our narrator. in the course of figuring it out, he becomes one of the sides mentioned. possible plot device: he has a corrupted copy?

# Misc. haiku

Down on the grasses,
I gaze at the summer sun -
And it gazes back!

Death poems are all just
falling blossoms and nonsense -
dying is dying

# Somatic genetic engineering

What's the killer app for non-medical genetic engineering in humans?

How about germ-line engineering of hair color? think about it. hair color is controlled by relatively few, and well-understood, genes. hair color is a dramatic change. there is massive demand for hair dye as it is, even with the extra effort and impermanence and unsatisfactory results. how many platinum blonds would jump at the chance to have kids who are *truly* enviably blond? or richly red-headed (and not washed-out Irish read)? A heck of a lot, I'd say. The health risks need not be enormous - aside from the intervention itself, what risk could swapping a brunette gene for blond cause? (There apparently is just 1 relevant gene: "Frost's theory is also backed up by a separate scientific analysis of north European genes carried out at three Japanese universities, which has isolated the date of the genetic mutation that resulted in blond hair to about 11,000 years ago." <http://www.timesonline.co.uk/tol/news/uk/article735078.ece>)

What sort of market could we expect? [Demographics of the United States](!Wikipedia)
103,129,321 women between 15 and 64; these are women who could be using dye themselves, so appreciate the benefit, and are of child-bearing years.

Likely, the treatment will only work if there's natural variation to begin with - that is, for Caucasians only. We'll probably want to exclude Hispanics and Latin Americans, who are almost as homogeneous in hair color as blacks and Asians, so that leaves us 66% of the total US population. 66% * 103,129,321 will get us a rough estimate of 6.806535186e7 or 68,065,351.

<http://www.isteve.com/blondes.htm> claims that "One study estimated that of the 30% of North American women who are blonde, 5/6^ths^ had some help from a bottle." (0.3 * (5/6) = 0.25 or 25%) says
[Demographics of Mexico](!Wikipedia) says 53,013,433 females
[Canada 2006 Census#Age and sex](!Wikipedia) 16,136,925

or 172,279,679 when you sum up Mexico/Canada/USA (the remaining NA states are too small to care about); 25% of 172,279,679 is 43,069,919. 43 million dye users.

Here's a random report <http://www.researchandmarkets.com/reportinfo.asp?report_id=305358> saying hair dye is worth 1 billion USD a year. Let's assume that this is all consumed domestically by women. (So 1,000,000,000 / 43,069,919 per year is 23)

A woman using hair dye on a permanent basis will be dying every month or so, or 12 times a year. Assume that one dye job is ~20 USD* (she's not doing it herself); then ((1b / 20) / 12) gives us ~4,166,666 women using hair dye, or 1/24 or 4.1% of eligible women. This seems rather low to me, based on observations, but I suppose it may be that elderly women do not use much hair dye, or the trend to using highlights and less-than-complete dye jobs. But 4% seems like a rather safe lower end. That's a pretty large market - 4 million potential customers, who are regularly expressing their financial commitment to their desire to have some hair color other than their natural one.

If each is spending even 100$ a year on it, a genetic engineering treatment could pay for itself very quickly. At 1000$, just 10 years. (And women can expect to live ~80). Not to mention, one would expect the natural hair to simply look better than the dye job.

There's a further advantage to this: it seems reasonable to expect that early forms of this sort of therapy will simply not work for minorities such as blacks or Hispanics - their markets wouldn't justify the research to make it work for them; their dark hair colors seem to be very dominant genetically, and likely the therapy would be working with recessive alleles (at least, it seems intuitively plausible that there is less 'distance' between making a Caucasian embryo, who might even have a recessive blonde allele already, fully blond, as compared to making a black baby, who would never ever come anywhere near a non-black hair color, blond). So marketing would benefit from an implicit racism and classism: racism in that one might need to be substantially Caucasian to benefit, and classism to be able to pony up the money up front.

* I think this price is a low-ball estimate by at least 50%; hopefully it will give us a margin of error, since I'm not sure how often dye-jobs need to be done.

# Games with a purpose

There doesn't seem to be any good method of crowd-sourcing translation, despite excellent tools like Google Translate. Perhaps there could be a variant on the [ESP Game](!Wikipedia)? Not entirely sure how it works, but: use Google Translate as a base line, and compete to improve it? or maybe, the players could be given a word, then a sentence, then a paragraph?

# Esoteric story of _Aria_

See [_Aria_'s past, present, and future](Aria's past, present, and future).

# The Camel Has Two Humps

Why does the camel have 2 humps? <http://www.cs.mdx.ac.uk/research/PhDArea/saeed/paper1.pdf> "All teachers of programming find that their results display a 'double hump'. It is as if there are two populations: those who can, and those who cannot, each with its own independent bell curve." tho Alan Kay seems a little skeptical <http://www.secretgeek.net/camel_kay.asp> and replications of the test have had issues; from <http://crpit.com/confpapers/CRPITV78Bornat.pdf>:

> "We now report that after six experiments, involving more than 500 students at six institutions in three countries, the predictive effect of our test has failed to live up to that early promise."

And <http://www.eis.mdx.ac.uk/research/PhDArea/saeed/SD_PPIG_2009.pdf>

> "A test was designed that apparently examined a student's knowledge of assignment and sequence before a first course in programming but in fact was designed to capture their reasoning strategies. An experiment found two distinct populations of students: one could build and consistently apply a mental model of program execution; the other appeared either unable to build a model or to apply one consistently. The first group performed very much better in their end-ofcourse examination than the second in terms of success or failure. The test does not very accurately predict levels of performance, but by combining the result of six replications of the experiment, five in UK and one in Australia. We show that consistency does have a strong effect on success in early learning to program but background programming experience, on the other hand, has little or no effect."

There's something to this; your first computer language is really hard no matter your experience but the second is almost trivial, unless it's a truly alien paradigm

This suggests some questions to me. Obviously on a raw information level, a natural language is *much* more complex than a computer language (the former are almost indefinitely complex with vocabulary, and the latter are engineered to be simple). Is it, relatively speaking, easier to learn a second natural language, or a second computer language? That is, if the difficulty of learning a second computer language is perhaps 10% of learning the first language, is that better or worse than the difficulty of learning a second natural language after one's native language? My own impression is that learning Haskell after I knew some Java was a lot easier than my first attempt at learning Haskell; when I learned some French after learning haskell, it seemed easier than before but not *that* much easier.

If this is so, it suggests that computer languages share more deep similarities than natural languages.

what is the knack of programming? Why do people never seem to cease being programmers - what irreversible paradigm shift happens in their heads?

# _The Peace War_ game

> "Tellman initialized the Celest board to level nine, Rosas noticed. The kid studied the setup with a calculating look. Tellman's display was a flat, owing a hypothetical solar system as seen from above the plane of rotation. The three planets were small disks of light moving around the primary. Their size gave a clue to mass, but the precise values appeared near the bottom of the display. Departure and arrival planets moved in visibly eccentric orbits, the departure planet at one rev every five seconds — fast enough so precession was clearly occurring. Between it and the destination planet moved a third world, also in eccentric orbit. Rosas grimaced. No doubt the only reason Tellman left the problem coplanar was that he didn't have a holo display for his Celest. Mike had never seen anyone without a symbiotic processor play the departure/destination version of Celest at level nine. The timer on the display showed that the player — the kid — had ten seconds to launch his rocket and try to make it to the destination. From the fuel display, Rosas was certain that there was not enough energy available to make the flight in a direct orbit. A cushion shot on top everything else!
>
> The kid laid all his bank notes on the table and squinted at the screen. Six seconds left. He grasped the control handles and twitched them. The tiny golden spark that represented his spacecraft fell away from the green disk of the departure world, inward toward the yellow sun about which all revolved. He had used more than nine-tenths of his fuel and had boosted in the wrong direction. The children around him murmured their displeasure, and a smirk came over Tellman's face. The smirk froze:
>
> As the spacecraft came near the sun, the kid gave the controls another twitch, a boost which — together with the gravity of the primary-sent the glowing dot far out into the mock solar system. It edged across the two-meter screen, slowing the greater remove, heading not for the destination planet but for the intermediary. Rosas gave an low, involuntary whistle. He had played Celest, both alone and with a processor. The game was nearly a century old and almost as popular as chess; it made you remember what the human race had almost attained. Yet he had never seen such a two-cushion shot by an unaided player.
>
> Tellman's smile remained but his face was turning a bit gray. The vehicle drew close to the middle planet, catching up to it as it swung slowly about the primary. The kid made barely perceptible adjustments in the trajectory during the closing period. Fuel status on the display showed 0.001 full. The representation of the planet and the spacecraft merged for an instant, but did not record as a collision, for the tiny dot moved quickly away, going for the far reaches of the screen.
>
> Around them, the other children jostled and hooted. They smelled a winner, and old Tellman was going to lose a little of the money he had been winning off them earlier in the day. Rosas and Naismith and Tellman just watched and held their breaths. With virtually no fuel left, it would be a matter of luck whether contact finally occurred.
>
> The reddish disk of the destination planet swam placidly along while the mock spacecraft arced higher and higher, slower and slower, their paths becoming almost tangent. The craft was accelerating now, falling into the gravity well the destination, giving the tantalizing impression of success that always comes with a close shot. Closer and closer. And the two lights became one on the board.
>
> "Intercept," the display announced, and the stats streamed across the lower part of the screen. Rosas and Naismith looked at each other. The kid had done it"

--Vernor Vinge, _The Peace War_

Visual presentation: basic problem, how to represent 4D trajectories, since players simply can't be given all the necessary information, must be computed. The trajectories are orbital paths. Plot paths as visible lines, with *color*: time is represented as shades of color, from red->purple. Each time unit is one pixel change, for example. 2 lines/paths intersect/collide only if they have the same color when crossing. Perhaps color intersections black or white to denote collision or miss? (And grey to indicate near-miss? Or closeness of approach?)

# Who wrote the _Death Note_ script?

So recently (October 2009) there appeared online a PDF file claiming to be a script for the Hollywood remake of the _[Death Note](!Wikipedia)_ anime (see Wikipedia, or my own little [Death Note Ending]() essay, for a description). Such a leak begs the question, is it genuine?

I was skeptical at first - how many unproduced screenplays get leaked? it's rare even in this Internet age - so I downloaded a copy and read it.

The first thing I noticed was that the 2 claimed authors, "Charley and Vlas Parlapanides, were correct: they were the 2 brothers of whom it had been quietly [announced](http://www.variety.com/article/VR1118003063.html) in April 2009 that they were hired to write it.

The second thing I did was take a look at the metadata. The creator tool checks out: "DynamicPDF v5.0.2 for .NET" is part of a commercial suite, and it was pirated well before July 2009, although I could not figure out when the commercial release was.

The date, though, is "Thu 09 Apr 2009 09:32:47 PM EDT". Keep in mind, this leak was October, and the original announcement was 30 April or so. If one were faking such a script, wouldn't one through either sheer carelessness & omission or by natural assumption (the Parlapanides signed a contract, the press release went out, and they started work) set the date well after the announcement? Why would you set it close to a month before? Wouldn't you take pains to show everything is exactly as an outsider would expect it to be? As Borges writes in "The Argentine Writer and Tradition":

> "Gibbon observes [in the _[Decline and Fall of the Roman Empire](!Wikipedia)_] that in the Arab book _par excellence_, the Koran, there are no camels; I believe that if there were ever any doubt as to the authenticity of the Koran, this lack of camels would suffice to prove it Arab. It was written by Mohammed, and Mohammed as an Arab had no reason to know that camels were particularly Arab; they were for him a part of reality, and he had no reason to single them out, while the first thing a forger or tourist or Arab nationalist would do is to bring on the camels - whole caravans of camels on every page; but Mohammed, as an Arab, was unconcerned. He knew he could be Arab without camels."

Another small point is the 'EDT', or Eastern Daylight-savings Time. The Parlapanides have long been based out of New Jersey

Then there is the corporate address quietly listed at the bottom of the page. It is widely available on Google if you can search for it, but one has to know about it in the first place. Easier to just leave it out. Another interesting detail.

What of the actual play? Well, it is written like a screenplay, properly formatted, and the scene descriptions are brief but occasionally detailed like the other screenplays I've read (such as the ''Star Wars'' trilogy's scripts). It is quite long and detailed. I could easily see a 2 hour movie being filmed from it. There are no obvious red flags.

The plot is curious. Ryuk and other [shinigami](!Wikipedia) are entirely omitted. Light is renamed 'Luke', and now lives in New York City, already in college. (Again, an appropriate setting for 2 screenwriters who grew up in New Jersey.) The plot is generally simplified.

What is more interesting is the changed emphases. Luke has been given a murdered mother, and much of his efforts go to tracking down the murderer (who, of course, escaped conviction for that murder). The Death Note is unambiguously depicted as a tool for evil, and a malign influence in its own right. There is minimal interest in the idea that Kira might be good. The Japanese aspects are minimized and treated as exotic curios, in the worst Hollywood tradition (Luke goes to a Japanese acquaintance for a translation of the kanji for 'shinigami', who, of course, being a primitive native, shudders in fear and flees the memsahib... oh, sorry, wrong era. But the description is still accurate.)

The ending shows Luke using the memory-wiping gambit to elude L (who from the script seems much the same, although things not covered by the script, such as casting, will be critically important to making L, L), and finding the hidden message from his old self - but destroying the message before he learns where he had hidden the Death Note. It is implied that Luke has redeemed himself, and L is letting him go. So the ending is classic Hollywood pap.

The ending indicates someone who doesn't love DN for its shades of gray mentality, its constant ambiguity and complexity. Any DN fan feels deep sympathy for Light, even if they root for L and co. I suspect that if they were to pen a script, the ending would be of the 'Light wins everything' variety, and not this hackneyed sop. I know I couldn't bring myself to write such a thing, even as a parody of Hollywood.

In general, the dialogue is short and hackneyed. There are no excellent megalomaniac speeches about creating a new world; one can expect a dearth of ominous choral chanting in the movie. Even the veriest tyro of fanfiction could write more DN-like dialogue than this script did.

Further, the complexities of ratiocination are largely absent, remaining only in the TV trick of L and the famous chips scene by Light. The tricks are even written incompetently - as written, on the bus, the crucial ID is seen by accident, whereas in DN, Light had written in the ID quite specifically. The moral subtlety of DN is gone; you cannot argue that Luke is a new god like Light. He is only an angry boy with a good heart lashing out, but by the end he has returned to the straight and narrow of conventional morality.

So much for the inside evidence; all suggestive, none damning. A forger *could* have randomly changed Charles to Charlie, looked up an appropriate address, edited the metadata, come up with all the Hollywood touches, wrote the whole damn thing (quite an endeavour since relatively little material is borrowed from DN), and put it online.

But is there any external evidence? Well, the timeline is right. Figure about 2 months for both brothers to read through the DN manga or watch the anime twice, clear up their other commitments, a month to brainstorm, 3 months to write the first draft, a month to edit it up and run it by the studio, and we're at 7 months or around February 2009. That leave a good 6 months for it to float around offices and get leaked, and then come to the wider attention of the Internet.

And then there is the fact that Warner Brothers has filed multiple take-down notices for hosts of the script. Not the 2 brothers, who would have a legal right to order the take-down of material falsely attributed to them, but the commissioning studio. Needless to say, they do not have a standing RIAA-style war against DN fanfiction or fan-art or even torrents of the anime or scanlations of the manga; just this script. Arguably, if the script were not the studio's property, it wouldn't have any legal ground to demand take-downs - their license likely covers just the movie rights, and so fanfiction in the form of a script (for example) would infringe on the Japanese rights-holder, not the studio.

I find this external legal argument fairly compelling, and in conjunction with the internal evidence and oddities best explained by the leaked script being authentically by the Hollywood scriptwriters, I've come to believe the script real. Perhaps an early draft, but still genuine. I suppose an American DN movie could be much much worse; just consider _[Dragon Ball Evolution](!Wikipedia)_!

# The advantage of an uncommon name

Theory: as time passes, it becomes more and more costly to have a 'common' name: a name which frequently appears either in history or in born-digital works. In the past, having a name like 'John Smith' may have not been a disadvantage - connections were personal, no one confused one John Smith with another, textual records were only occasionally used. It might sometimes be an issue with bureaucracy such as taxes or the legal system, but nowhere else.

But online, it is important to be findable. You want your friends on Facebook to find you with the first hit. You want potential employers doing surreptitious Google searches before an interview to see your accomplishments and not others' demerits; you do not want, as Abigail Garvey discovered when she married a Wilson, [employers thinking your resume fraudulent](http://online.wsj.com/article/SB117856222924394753.html) because you are no longer ranking highly in Google searches. As Kevin Kelly has since [put it](http://www.kk.org/thetechnium/archives/2011/02/google-unique_n.php):

> "With such a common first/last name attached to my face, I wanted my children to have unique names. They were born before Google, but the way I would put it today, I wanted them to have Google-unique names."

[Clive Thompson](!Wikipedia) [says](http://www.collisiondetection.net/mt/archives/2007/05/_in_the_age_of.php) that search rankings were why he originally started blogging:

> "Today's search engines reward people who have online presences that are well-linked-to. So the simplest way to hack Google to your advantage is to blog about something you find personally interesting, at which point other people with similar interests will begin linking to you — and the upwards cascade begins.
>
> This is precisely one of the reasons I started Collision Detection: I wanted to 0wnz0r the search string “Clive Thompson”. I was sick of the British billionaire and Rentokil CEO Lord Clive Thompson getting all the attention, and, frankly, as a freelance writer, it's crucially important for anyone who wants to locate me — a source, an editor, old friends — to be able to do so instantly with a search engine. Before my blog, a search for “Clive Thompson” produced a blizzard of links dominated by the billionaire; I appeared only a few times in the first few pages, and those were mostly just links to old stories I'd written that didn't have current email addresses. But after only two months of blogging, I had enough links to propel my blog onto the first page of a Google search for my name."

This isn't obvious. It's easy to raise relatively rare risks as objections (but how many cases of identity theft are made possible solely by a relatively unique name making a person google-able? Surely few compared to the techniques of mass identity theft: corporate espionage, dumpster diving, cracking, skimming etc.) To appreciate the advantages, you have to be a 'digital native'. Until you've tried to Google friends or acquaintances, the hypothesis that unique names might be important will never occur to you. Until then, as long as your name was unique inside your school classes, or your neighborhood, or your section of the company, you would never notice. Even researchers spend their time researching unimportant correlations like people named Baker becoming bakers more often, or people tending to move to a state whose name they share (like Georgia).

What does one do? One avoids as much as possible choosing any name which is in the say, top 100 most popular names. People with especially rare surnames may be able to get away with common personal names, but not the Smiths. (It's easy to check how common names are with [online tools](http://howmanyofme.com/search/) drawing on US Census data. My own name pair is unique at the expense of the Dutch surname being 12 letters long, and difficult to remember.)

But one doesn't wake up and say "I will name myself Zachariah today because John is so damn common". After 20 years or more, one is heavily invested in one's name. It's acceptable to change one's surname (women do it all the time), but not the first name.

One *does* decide the first name of one's children, though, and it's iron tradition that one does so. So we can expect digital natives to shy away from common names when naming their kids. But remember who are the 'digital natives' - kids and teenagers of the '00s, at the very earliest. If they haven't been on, say, Facebook for years, they don't count. Let's say their ages are 0-20 during 2008 when Facebook really picked up steam in the non-college population; and let's say that they won't have kids until ~30. The oldest of this cohort will reach child-bearing age at around 2018, and every one after that can be considered a digital native from osmosis if nothing else. 2018 is when we will see a growing '[long tail](!Wikipedia "Heavy-tailed distribution")' of baby names.

So this is a good story: we have a suboptimal situation (too many collisions in the new global namespace of the Internet) and a predicted adjustment with specific empirical consequences.

But there are issues.

- Rare names may come with comprehensibility issues; [Zooko's triangle](!Wikipedia) in cryptography says that names cannot be unique, globally valid, *and* short or human-meaningful. You have to compromise on some aspect.
- There's already a decline in popular names, according to [Wikipedia](!Wikipedia "Given names#Popularity distribution of given names"):

    > "Since about 1800 in England and Wales and in the U.S., the popularity distribution of given names has been shifting so that the most popular names are losing popularity. For example, in England and Wales, the most popular female and male names given to babies born in 1800 were Mary and John, with 24% of female babies and 22% of male babies receiving those names, respectively. In contrast, the corresponding statistics for in England and Wales in 1994 were Emily and James, with 3% and 4% of names, respectively. Not only have Mary and John gone out of favor in the English speaking world, also the overall distribution of names has changed significantly over the last 100 years for females, but not for males."

    (The female trend has continued through to 2010: "The 1,000 top girl names accounted for only 67 percent of all girl names last year, down from 91 percent in 1960 and compared with 79 percent for boys last year."^[["Say Goodnight, Grace (and Julia and Emma, too)"](https://www.nytimes.com/2011/06/26/magazine/the-state-of-babies-names-hello-jayden-goodbye-hannah.html). _New York Times Magazine_]) The theory could probably be rescued by saying that the advantage of having a unique given name (and thus a relatively unique full name) goes that far back, but then we would need to explain why the advantage would be there for women, but *not* men.
- Pop culture is known to have a very strong influence on baby names (cf. the popularity of _Star Wars_ and the subsequent massive spike in 'Luke'). The counter-arguments to [The Long Tail](!Wikipedia) marketing theory say that pop culture is becoming ever more monolithic and hit-driven. The fewer hits, and the more mega-hits, the more we could expect a few names to spike and drive down the rate of other names. The effect on a rare name can be incredible even from relatively small hits (the song in question was only a Top 10):

    > "Kayleigh became a particularly popular name in the United Kingdom following the release of a song by the British rock group Marillion. Government statistics in 2005 revealed that 96% of Kayleighs were born after 1985, the year in which Marillion released "Kayleigh"."^[Wikipedia again]
- Given names follow a power-law distribution already where a few names dominate, and so small artifacts can make it appear that there is a shift towards unpopular names. Immigration or ethnic groups can distort the statistics and make us think we see a decline in popular names when we're actually seeing an increase in popular names elsewhere - imagine all the Muhammeds and Jesuses we might see in the future. Those will show up as decreases in the percentages of 'John' or 'James' or 'Emily' or 'William', and fool us, even though Muhammed and Jesus are 2 of the most popular names in the world.

(The above appears to be pretty common knowledge among people interested in baby names and onomastics in general; for example, a _Washington Post_ editorial by Laura Wattenberg, "Are our unique baby names that unique?", 16 Sunday May 2010, argues much of the above.)

# Optimizing the alphabet

Here's an interesting idea: the glyphs of the Phoenician-style alphabet are not optimized in any sense. They are bad in several ways, and modern glyphs are little better. For example, v and w, or m and n. People confuse them all the time, both in reading and in writing.

So that's one criterion: glyphs should be as distinct from all the rest as possible.

What's a related criterion? m and w are another pair which seem suboptimal, yet they are as dissimilar as, say, a and b, under many reasonable metrics. m and w are related via *symmetry*. Even though they share relatively few pixels, they are still identical under rotation, and we can see that. We could confuse them if we were reading upside down, or at an angle, or just confuse them period.

So that's our next criterion: the distinctness must also hold when the glyph is rotated by any degree and then compared to the rest.

OK, so we now have a set of unique and dissimilar glyphs that are unambiguous about their orientation. What else? Well, we might want them to be easy to write as well as read. How do we define 'easy to write'? We could have a complicated physiological model about what strokes can easily follow what movements and so on, but we will cop out and say: it is made of as few straight lines and curves as possible. Rather than unwritable pixels in a grid, our primitives will be little geometric primitives.

The fewer the primitives and the closer to integers or common fractions the positioning of said primitives, the simpler and the better.

We throw all these rules in, add a random starting population or better yet a population modeled after the existing alphabet, and begin our genetic algorithm. What 26 glyphs will we get?

Problem: our current glyphs may be optimal in a deep sense:

> Dehaene describes some fascinating and convincing evidence for the first kind of innateness. In one of the most interesting chapters, he argues that the shapes we use to make written letters mirror the shapes that primates use to recognize objects. After all, I could use any arbitrary squiggle to encode the sound at the start of “Tree” instead of a T. But actually the shapes of written symbols are strikingly similar across many languages.
> It turns out that T shapes are important to monkeys, too. When a monkey sees a T shape in the world, it is very likely to indicate the edge of an object — something the monkey can grab and maybe even eat. A particular area of its brain pays special attention to those significant shapes. Human brains use the same area to process letters. Dehaene makes a compelling case that these brain areas have been “recycled” for reading. “We did not invent most of our letter shapes,” he writes. “They lay dormant in our brains for millions of years, and were merely rediscovered when our species invented writing and the alphabet.”
<http://www.nytimes.com/2010/01/03/books/review/Gopnik-t.html>

# On lying and not lying

A small gem of equivocation:

An old and somewhat estranged family friend abandoned 2 cats with us when she went to seek her fortune in the West (turned out her brother there was only offering her room because he hoped to get her kidney); the cats lived relatively happily with us until one day, the black one made the mistake of taking a nap behind a wheel. Backing up, my mother ran it over. Yowling, he ran into the garage where we had kept them early on, and in a corner, expired of its injuries.

6 weeks later, the friend called and asked for news of that cat. My mother had previously consulted with her sister and replied - very carefully - 'We found it dead in the garage.'

# On promises

When I was in elementary school, another family friend was named Patti, with sons Nick & Joe (the husband was not apparent). They were perhaps middle school aged, but we got along well, I thought. They had an interesting house. It was by the fire station, roughly in the same part of town (Centereach) as my old blue house. That propinquity and Patti's Dutch heritage explain the original connection, I think. Their (rented) house had a large piece of land, and a U-shaped driveway that went through the front. In the middle was a veritable island-mountain, with a giant pine in the middle. Underneath it was a mass of boulders poking through the thick drifts of needles. I had a Swiss army knife, and delighted in scraping sparks against the stone.

Behind the small red house was an orchard in advanced desuetude. I only ever noticed grapes in its arbors. They were purple, I think, and utterly untended. They were bitter - very foxy. The previous owner had loved grapes.

Joe was older. He liked video games, I remember. At this time in the '90s, there was only Nintendo & Sega with oddball also-rans like Lynx or Neo-Geo that kids like us scoffed at. The distinction was that Sega was known for its capable hardware and more adult games, but a smaller overall game library, and Nintendo was known for its odd controllers, its 1st-party games, and a large library of games (I understand that the SNES game library would only be surpassed only in the 2000s by the PS2 with its backwards compatibility). Joe was a Sega fan, and a diehard one - he demonstrated to me that he had bought the ill-starred 'Mega-CD' and also the poor Sega Saturn, though he had little to play on them but a _Sonic the Hedgehog_ game.

One year, we took Nick & Joe with us down south to visit our grandparents & Washington D.C. with our customary visits to the Smithsonian Museum of Natural History and the Air & Space Museum. We stayed at the Birch townhouse in Richmond, Virginia. This was a neighborhood of townhouses, with a genteel air and many pines behind the rows of nigh-identical townhouses. There was a little park not far from us. It wasn't used much (there were few enough children in the area) and one day Joe and I had gone there - kicked out from the TV and the flat, I recall, by an adult - and were skirmishing & discussing Taekwondo in the silly boastful way kids will who watched too much _Teenage Mutant Ninja Turtles_. Joe claimed to know a great deal about the martial arts, but he said he could not teach me; I and Allison were but green belts and not ready. But, he said, in a few years when I had become a brown (or, hazy memory avers, red) belt, then he would teach me.

A few years later, I had persevered (Allison had stopped) and reached the agreed-upon rank. But by then Patti had ceased to be a good family friend and the last I heard of Joe was in some military.

# Meta

A: But who is to say that a butterfly could not dream of a man? You are not the butterfly to say so!
B: No. Better to ask what manner of beast could dream of a man dreaming a butterfly, and a butterfly dreaming a man.

# Why IQ doesn't matter and how points mislead

One common anti-IQ arguments is that IQ does nothing and may be actively harmful past 120 or 130 or so; the statistical evidence is there to support a loss of correlation with success, and commentators can adduce [William Sidis](!Wikipedia) if they don't themselves know any such 'slackers', or the [Terman report](!Wikipedia)'s similar findings.

This is a reasonable objection. But it is rarely proffered by people really familiar with IQ, who also rarely respond to it. Why? I believe they have an intuitive understanding that IQ is a *percentile ranking*, not an *absolute measurement*.

It is plausible that the 20 points separating 100 and 120 represents far more cognitive power and ability than that separating 120 and 140, or 140 and 160. To move from 100 to 120, one must surpass roughly 20% of the population; to move from 120 to 140 requires surpassing a smaller percentage, and 140-160 smaller yet.

Similarly it should make us wonder how much absolute ability is being measured at the upper ranges when we reflect that, while adult IQs are stable over years, they are unstable in the short-term and test results can vary dramatically even if there is no distorting factors like emotional disturbance or varying caffeine consumption.

Another thought: the kids in your local [special ed](!Wikipedia) program mentally closer to chimpanzees, or to Albert Einstein/[Terence Tao](!Wikipedia)? Pondering all the things we expect even special ed kids to learn (eg. language), I think those kids are closer to Einstein than monkeys.

And if retarded kids are closer to Einstein that the smartest non-human animal, that indicates human intelligence is very 'narrow', and that there is a vast spectrum of stupidity stretching below us all the way down to viruses (which only 'learn' through evolution).

A gap like 20 points looks very impressive from our compressed human perspective, but it reflects very little absolute difference; to a sheep, other sheep are each distinctive. In [Big O](!Wikipedia) computer terms, we might say that geniuses are a [constant factor](!Wikipedia) faster than their dimmer brethren, but not [asymptotically](!Wikipedia) so.

It is expected then, that someone measured at 180 doesn't make the rest of us look like a retard of 20. To be so smart requires thousands of factors (mental & biological) to click just right; if ordinary people luck out on 900 factors, then those geniuses' scores are trying to secern differences of 2 or 3 factors, and the practical impact of a few factors out of thousands may be minimal and explain the findings without denying the existence of such differences.

# Breakfast

My little sister Molly went to Hawkin's Path Elementary much the same as all of us did. Early on, one morning before classes began, she went to the cafeteria and got on the line for the breakfasts. Pretty much the only people who ate breakfast at Hawkin's Path were the (very) poor kids who qualified for the Federal free breakfast program. Molly, as it happens, was neither on the list nor had a card. But the lunch lady was new to the job & school, so when her turn came up, she said, "I'm Molly!" with such straightforwardness and assurance - as though *of course* that explained everything, who could not know about Molly? - that the flummoxed lady simply gave her a breakfast.

Some time later, Molly's kindergarten teacher Mrs. MacNamara would approach Mom & Dad at a book fair (or something) and shock them by inquiring as to whether Molly qualified for the program.

# True dreams

One curious event, that well illustrates the uncanny hold that coincidences can exert on our minds, is worth recording. One day, I had read part of Frank Herbert's _Dune_ and run into the word '[Spannungsbogen](!Wiktionary)' - a kind of self-discipline or restraint as Herbert described it. The word had an entry in [Wiktionary](!Wikipedia) but the entry lacked sources & examples, and was at risk of deletion. So I went to one of my favorite sources - _The New York Times_ - and searched their archives, found one useful hit (later, I would not remember anything about what the hit said; just that it existed), and listed it on the talk page (since I'm not familiar with Wiktionary conventions and prefer to let the regulars integrate new references into entries).

Then I woke up.

Some time later, I remembered the dream and thought to myself that I ought to check whether I had not actually added it yesterday and was mis-remembering; I had not - the talk page was devoid of my contribution - but the entry did need work. I then thought it would be amusing to see what the NYT *did* have, so I went and searched - and found one useful hit. Disquieted, I [edited the talk page](http://en.wiktionary.org/w/index.php?title=Talk%3ASpannungsbogen&action=historysubmit&diff=7043672&oldid=3348973) as in my dream, and moved on.

# Backups: life and death

Consider the plight of an upload - a human mind running on a computer rather than a brain. It has the advantage of all digital data: perfect fidelity in replication, fast replication - replication period. An upload could well be immortal. But an upload is also very fragile. It needs storage at every instance of its existence, and it needs power for every second of thought. It doesn't carry with it any reserves - a bit is a bit, there are no bits more durable than other bits, nor bits which carry small batteries or [UPSes](!Wikipedia "Uninterruptable power supply") with themselves.

So reliable backups are literally life and death for uploads.

But backups are a double-edged sword for uploads. If I backup my photos to [Amazon S3](!Wikipedia) and a bored employee pages through them, that's one thing; annoying or career-ending as it may be, pretty much the worst thing that could happen is that I get put in jail for a few decades for child pornography. But for an upload? If an enemy got a copy of its full backups, the upload has essentially been kidnapped. The enemy can now run copies and torture them for centuries, or use them to attack the original running copy (as hostages, in [false flag attacks](!Wikipedia), or simply to understand & predict what the original will do). The negative consequences of a leak are severe.

So backups need to be both reliable and secure. These are conflicting desires, though.

One basic principle of long-term storage is '[LOCKSS](!Wikipedia)': "lots of copies keeps stuff safe". Libraries try to distribute copies of books to as many holders as possible, on the premise that each holder's failure to preserve a copy is a random event independent of all the other holders; thus, increasing the number of holders can give arbitrarily high assurances that *a* copy will survive. But the more copies, the more risk one copy will be misused. That's fine if 'misuse' of a book is selling it to a book collector or letting it rot in a damp basement; but 'misuse' of a conscious being is unacceptable.

Suppose one encrypts the copies? Suppose one uses a [one-time pad](!Wikipedia), since one worries that an encrypted copy which is bullet-proof *today* may be copied and saved for centuries until the encryption has been broken, and is perfectly certain the backups are 'secure'. Now one has 2 problems: making sure the backups survive until one needs them, and making sure the one-time pad survives as well! If the future upload is missing either one, nothing works.

The trade-off is unfortunate, but let's consider secure backups. The first and most obvious level is physical security. Most systems are highly vulnerable to attackers who have physical access; desktop computers are trivially hacked, and [DRM](!Wikipedia) is universally a failure.

Any backup ought to be as inaccessible as possible. [Security through obscurity](!Wikipedia) might work, but let's imagine *really* inaccessible backups. How about hard drives in orbit? No, that's too close: commercial services can reach orbit easily, to say nothing of governments. And orbit doesn't offer too much hiding space. How about orbit not around the Earth, but around the Solar System? Say, past the orbit of Pluto?

That offers an enormous volume: the Kuiper Belt is roughly ~1.95^30^ cubic kilometers[^volume]. The lightspeed delay is at least 20 minutes, but [latency](!Wikipedia) isn't an issue; a backup protocol on Earth could fire off one request to an orbiting device and the device would then transmit back everything it stored without waiting for any replies or confirmations (somewhat like [UDP](!Wikipedia)).

10^30^ cubic kilometers is more than enough to hide small stealthy devices in. But once it sends a message back to Earth, its location has been given away - the Doppler effect will yield its velocity and the message gives its location at a particular time. This isn't enough to specify its orbit, but it cuts down significantly on where the device could be. 2 such messages and the orbit is known. A restore would require more than 2 messages.

The device could self-destruct after sending off its encrypted payload. But that is very wasteful. We want the orbit to change unpredictably after each broadcast.

If we imagine that at each moment the device chooses between firing a thruster to go 'left' or 'right', then we could imagine the orbit as being a message encrypted with a one-time pad - a one-time pad, remember, being a string of random bits. The message is the original orbit; the one-time pad is a string of random bits shared by Earth and the device. Given the original orbit, and knowing when and how many messages have been sent by the device, Earth can compute what the new orbit is and where the device will be in the future. ('It started off on this orbit, then the random bit-string said at time X to go left, then at X+1, go left again, then at X+Y, go right; remembering how fast it was going, that means it should now be... there in the constellation of Virgo.')

The next step up is a symmetric cipher: a shared secret used not to determine future orbit changes, but to send messages back and forth - 'go this way next; I'm going this way next; start a restore' etc. But an enemy can observe where the messages are coming from, and can work out that 'the first message must've been X, since if it was at point N and then showed up at point O, only one choice fits, which means this encrypted message meant X, which lets me begin to figure out the shared secret'.

A public-key system would be better: the device encrypts all its messages against Earth's private key, and vice versa. Now the device can randomly choose where to go and tell Earth its choice so Earth knows where to aim its receivers and transmitters next.

But can we do better?

[^volume]: The area of a sphere is given by the equation: $\frac{4}{3} \times \pi \times r^3$\
       1 AU = $149.60 \times 10^6$ kilometers\
       30 AU = $30 \times 149.60 \times 10^6$, or $4.488 \times 10^9$ km\
       55 AU = $55 \times 149.60 \times 10^6$, or $8.228 \times 10^9$ km\
       So the shell is the volume of the outer sphere minus the inner sphere:\
       $(\frac{4}{3} \times \pi \times (8.228 \times 10^9)^3) - (\frac{4}{3} \times \pi \times (4.488 \times 10^9)^3)$, or $1.9546466984296578 \times 10^{30}$.

# A secular humanist reads _The Tale of Genji_

After several years, I finished reading Edward Seidensticker's translation of _[The Tale of Genji](!Wikipedia)_. Many thoughts occurred to me towards the end, when the novelty of the Heian era began to wear off and I could be more critical.

The prevalence of poems & puns is quite remarkable. It is also remarkable how tired they all feel; in _Genji_, poetry has lost its magic and has simply become another stereotyped form of communication, as codified as a letter to the editor or small talk. I feel fortunate that my introductions to Japanese poetry have usually been small anthologies of the greatest poets; had I first encountered court poetry through _Genji_, I would have been disgusted by the mawkish sentimentality & repetition.

The gender dynamics are remarkable. Toward the end, one of the two then main characters becomes frustrated and casually has sex with a serving lady; it's mentioned that he liked sex with her better than with any of the other servants. Much earlier in _Genji_ (it's a good thousand pages, remember), Genji simply rapes a woman, and the central female protagonist, Murasaki, is kidnapped as a girl and he marries her while still what we would consider a child. (I forget whether Genji sexually molests her before the _pro forma_ marriage.) This may be a matter of non-relativistic moral appraisal, but I get the impression that in matters of sexual fidelity, rape, and children, Heian-era morals were not much different from my own, which makes the general immunity all the more remarkable. (This is the 'shining' Genji?) The double-standards are countless.

The power dynamics are equally remarkable. Essentially every speaking character is nobility, low or high, or Buddhist clergy (and very likely nobility anyway). The characters spend next to no time on 'work' like running the country, despite many main characters ranking high in the hierarchy and holding minister-level ranks; the Emperor in particular does nothing except party. All the households spend money like mad, and just expect their land-holdings to send in the cash. (It is a signal of their poverty that the Uji household ever even mentions how less money is coming from their lands than used to.) The Buddhist clergy are remarkably greedy & worldly; after the death of the father of the Uji household, the abbot of the monastery he favored sends the grief-stricken sisters a note - which I found remarkably crass - reminding them that he wants the customary gifts of valuable textiles.

The medicinal practices are utterly horrifying. They seem to consist, one and all, of the following algorithm: 'while sick, pay priests to chant.' If chanting doesn't work, hire more priests. (One freethinker suggests that a sick woman eat more food.) Chanting is, at least, not outright harmful like bloodletting, but it's still sickening to read through *dozens* of people dying amidst chanting. In comparison, the bizarre superstitions (such as trapping them in houses on inauspicious days) that guide many characters' activities are unobjectionable.

The 'ending' is so abrupt, and so clearly unfinished; many chapters have been spent on the 3 daughters of the Uji householder, 2 are disposed of, and the last one has just been discovered in her nunnery by 1 of the 2 protagonists (and the other protagonist suspects). The arc is not over until the would-be nun has been confronted, yet the book ends. Given that [Murasaki Shikibu](!Wikipedia) was writing an episodic entertainment for her court friends, and the overall lack of plot, I agree with Seidensticker that the abrupt mid-sentence ending is due either to Shikibu dying or abandoning her tale - not to any sort of deliberate plan.

# Measuring multiple times in a sandglass

How does one make a sand hourglass measure multiple times?

One could just watch it and measure fractions by eye - when a 10-minute timer is down to 1/2, it has measured 5 minutes. One could mark the outside and measure fractions that way.

Or perhaps one could put in two-toned sand - when the white has run out and there's only black sand, then 5 minutes has passed.

But the sand would inevitably start to mix, and then you just have a 10-minute timer with grey sand. Perhaps some sort of plastic sheet separating them? But it would get messed up when it passes through the funnel.

Then, perhaps the black sand could be magnetically charged positively, and the white sand negatively? But magnetism attracts *unlike*. If the black is positive and white negative, they'll clump together even more effectively than random mixing would.

We can't make a color homogeneous in charge. Perhaps we could charge just black negative, and put positive magnets at the roof and floor? The bias might be enough over time to counteract any mixing effect - the random walk of grains would have a noticeable bias for black. But if the magnet is strong, then some black sand would never move, and if it's weak, then most of the sand will never be affected; either way, it doesn't work well.

Perhaps we could make half the black sand positive and half negative, while all white is neutral? Black will clump to black everywhere in the hourglass, without any issues about going through the funnel or affecting white.

How might this fail? Well, why would there be only *2* layers? There could be several alternating layers of black and white, and this be a stable system.

We might be able to remedy this by combining magnetized black sand with magnets on the roof/floor, imparting an overall bias - the layers form, but slowly get compacted together.

The real question is whether strong enough magnetism to usefully sort is also so strong to clump together and defeat the gravity-based timing.

# Measuring social trust by offering free lunches

People can be awfully suspicious of free lunches. I'd like to try a little experiment or stunt sometime to show this. Here's how it'd go.

I'd grab myself a folding table, make a big poster saying 'Free Money! $1 or $2' and in fine print, 'one per person per day'. Then, anyone who came up and asked would get $2. Eventually, someone would ask for $1 - they would get it, but be asked first *why* they declined the larger amount.

I think their answers would be interesting.

Even funner would be giving the $2 as a 2-dollar bill, and not 2 dollar bills. They're rare enough that it would be quite a novelty to people.

# One lazy dog

Today I noted to my grandfather Gerald Birch that a lot of the most local Marylanders seem to have both pick-up trucks and dogs in the cabs. He told me an anecdote about his cousin Dill's life was saved by this habit.

One day Dill was driving down a road with his dog. For whatever reason, he goes wild into the woods and crashes and is knocked unconscious. His dog, more sensible than he, picked itself up and walked out to the road, where it then sat down and waited.

Eventually, some friends of Dill came driving by and recognized the dog instantly. "What's Dill's dog doing sitting there?" they asked one another as they stopped and got out. This led to them going into the woods where they found Dill in his wreck.

# Upsides to child abuse

Once, after a meeting of the RIT anime club, I got into a heated argument with another fellow. He and I had a running series of insults and arguments (often centering on how I had a problem with his face).

At some point, I asserted that 'there is nothing funny about child abuse!'

He begged to differ.

Very well, then, said I, tell me a funny child abuse joke. He craved 5 minutes, which I readily granted.

4 minutes in, he lifted his head from deep in thought and told me the following joke:

> "What's more fun than beating your child with a board game?"
>
> "I don't know."
>
> "Beating them with anything else."

I paused, and then conceded defeat.

# Revealed preferences

A teacher of mine, although for the life of me I cannot remember where or whom, once told the class a story about European customs.

When he was a younger man, he said, he once was in a restaurant in Amsterdam, when a fellow American walked in. She was a beautiful young woman and the teacher noticed her immediately, as did the virile waiters.

She walked to one of the bistro's tables and sat down, clearly expecting to be served. The waiters were greedily looking at her from over in their corner, but making no move.

It is the custom, the teacher explained, that in America, the waiter accosts the customer but vice versa in Europe. They were at an impasse. It was amusing, he said - one wanted to be served and the other to serve.

The young lady's impatience boiled over after a score of minutes, and she left, much to the dismay of the waiters. And all for want of a nail.

# Leaf burgers

One thing I was known for in Boy Scouts (or so I thought) was my trick of cooking hamburgers with leaves rather than racks or pans. I had learned it long ago at a campboree, and made a point of cooking my hamburger that way and not any other.

The way it works is you take several large green leafs straight from the tree, and sandwich your burger. Ideally you only need 2, one leaf on top and the other on bottom. (I was originally taught using just one leaf, and carefully flipping the burger on top of its leaf, but that's error prone - one bad flip and your burger is irretrievably dirty and burned.) Then you put your green sandwich on top of a nice patch of coals - no flames! - and flip it in 10 minutes or so.

You'll see it smoke, but not burn. The green leaves themselves don't want to burn, and the hamburger inside is giving off lots of water, so you don't need to worry unless you're overcooking it. At about 20 minutes, the leaves should have browned and you can pull it out and enjoy.

What's the point of this? Well, it saves one dishes. Given how difficult it is to clean dishes out there where there are no dishwashers or sinks, this should not be lightly ignored. It cooks better: much more evenly and with less char or burning of the outside. Given many scouts' cooking skills, this is no mean consideration either. It's a much more interesting way to cook. And finally, the hamburger ends up with a light sort of 'leafy' taste on the outside, which is quite good and not obtainable any way else.

# Priorities

Grandma Birch once recounted how uncle Tom had beautiful hair as a child which she refused to allow be cut; one day, Grandad Birch took him to have it cut anyway while she was away. That same day, uncle Tom was playing in the street when a local girl ran into him with her car, hurtling him back up onto the lawn and leaving a scar on his face that one can still see as a dimple.

When she returned, she remembers that her chief concern was what had been done to his beautiful hair!

# Things kids say

Aunt Sally recounted 2 stories:

One of her elementary school colleagues was named 'Thomas Magwood'. She asked him whether anyone had called him Maggy Maggot. He answered yes. She wondered which student. Magwood replied: 'My granddaughter.' Aunt Sally: 'Thomas, I was not expecting that answer.'

Another student had lost her father while young, and one day was asking where he had gone. Her mother eloquently spoke of how he now lived on in their hearts and would remain in their house forever. The child acquiesced, and some time later, announced that she had remembered her father's name. What is it, the mother asked. Quite firmly she replied: 'Jesus.'

# Night watch

> "The gloom of dusk. \
> An ox from out in the fields \
> comes walking my way; \
> and along the hazy road \
> I encounter no one."^[Shōtetsu; 59 'An Animal in Spring';  _Unforgotten Dreams: Poems by the Zen monk Shōtetsu_; trans. Steven D. Carter, ISBN 0-231-10576-2]

Night watch is not devoid of intellectual interest. The night is quite beautiful in its own right, and during summer, I find it superior to the day. It is cooler, and often windier. Contrary to expectation, it is less buggy than the day. Fewer people are out, of course.

My own paranoia surprises me. At least once a night, I hear noises or see light, and become convinced that someone is prowling or seeks to break in. Of course, there is no one there. This is true despite it being my 4th year. I reflect that if it is so for me, then what might it be like for a primitive heir to millennia of superstition? There is a theory that spirits and gods arise from overly active imaginations, or pattern-recognition as it is more charitably termed. My paranoia has made me more sympathetic to this theory. I am a staunch atheist, but even so!

The tempo at night varies as well. It seems to me that the first 2 years, cars were coming and going every night. Cars would meet, one would stay and the other go; or a car would enter the lot and not leave for several days (with no one inside); or they would simply park for a while. School buses would congregate, as would police-cars, sometimes 4 or 5 of them. In the late morning around 5 AM, the tennis players would come. Sometimes when I left at 8 AM, all 4 or 5 courts would be busy - and some of the courts hosted 4 players. I would find 5 or 6 tennis balls inside the pool area, and would see how far I could drop-kick them. Now, I hardly ever find tennis balls, since I hardly ever see tennis players. A night in which some teenagers congregate around a car and smoke their cigarettes is a rarity. Few visit my lot.

I wonder, does this have to do with the recession which began in 2008?

## Fiction

The night has, paradoxically, sights one cannot see during the day. What one can see takes on greater significance, becoming new and fresh. I recall one night long ago; on this cool dark night, the fogs lay heavy on the ground, light-grey and densely soupy. In the light, one could watch banks of fog swirl and mingle in myriads of meetings and mutations; it seemed a thing alive. I could not have seen this under the sun. It has no patience for such ethereal and undefinable things. It would have burned off the fog, driven it along, not permitted it to linger. And even had it existed and been visible, how could I have been struck by it if my field of view were not so confined?

One feels an urge to do strange things. The night has qualities all its own, and they demand a reflection in the night watcher. It is strange to be awake and active in the wrong part of the day, and this strangeness demands strangeness on one's own part. Often when doing my rounds I have started and found myself perched awkwardly on a bench or fence. I stay for a time, ruminating on nothing in particular. The night is indefinite, and my thoughts are content to be that way as well. And then something happens, and I hop down and continue my rounds.

For I am the sole inhabitant of this small world. The pool is bounded by blackened fences, and as it lies prostrate under tall towers bearing yellowed flood-lights. The darkness swallows all that is not pool, and returns a feeling of isolation. As if nothing besides remains. I circumnambulate to recreate the park, to assure me it abides, that it is yet there to meet my eyes - a sop to conscience, a token of duty; an act of creation.

I bring the morning.

# Two cows: philosophy

Philosophy [two-cows](!Wikipedia "You have two cows") jokes:

Free will: you have 2 cows; in an event entirely independent of all previous events & predictions, they devour you alive; this makes no sense as cows are herbivores, but you are no longer around to notice this.

Fatalism: you have 2 cows; whether they survive or not is entirely up to the inexorable and deterministic course of the universe, and what you do or not likewise, so you don't feed your cows and they starve to death; you reflect that the universe really has it in for you.

Compatibilism: you have 1 cow which is free and capable of making decisions, and 1 cow that is determined and bound to follow the laws of physics; they are the same cow. But you get 2 cows' worth of milk anyway.

Existentialism: You have two cows; one is a metaphor for the human condition. You kill the other and in bad faith claim hunger made you do it.

Ethics: You have two cows, and as a Utilitarian, does it suit the best interests of yourself and both cows to milk them, or could it be said that the interests of yourself, as a human, come above those of the cows, who are, after all, inferior to the human race? Aristotle would claim that this is correct, although Peter Singer would disagree.

Sorites: you have 2 cows who produce a bunch of milk; but if you spill a drop, it's still a bunch of milk; and so on until there's no more milk left. Obviously it's impossible to have a bunch of milk, and as you mope over how useless your cows are, you die of thirst.

Nagarjuna: You have 2 cows; they are 'empty', of course, since they are dependent on grass; you milk them and get empty-milk (dependent on the cow), which tastes empty; you sell them both and go get some real cows. _Moo mani hum_...

Descartes: You have 2 cows, therefore you are (since deceive me howbeit the demon may, he can never make it so that I have 2 cows yet am not); further, there are an infinite # of 2-cows jokes, and where could this conception of infinity have come from but God? Therefore he exists. You wish you had some chocolate milk.

Bentham: no one has a natural right to anything, since that would be '2 cows walking upon stilts'; everything must be decided by the greatest good for the greatest number; you get a lobotomy and spend the rest of your life happily grazing with your 2 cows.

Tocqueville: Cows are inevitable, so we must study the United Cows of America; firstly, we shall take 700 pages to see how this nation broke free of the English Mooarchy, and what factors contributes to their present demoocracy...

Gettier: You see 2 cows in your field - actually, what you see is 2 cow-colored mounds of dirt, but there really are 2 cows over there; when you figure this out, your mind is blown and >2000 years of epistocowlogy shatters.

Heidegger: [dasein](!Wikipedia) dasein apophantic being-in cow being-in-world milk questioning proximate science thusly Man synthesis time, thus, 2 cows.

Husserl: You have 2 cows, but do you really *see* them?

# Waking up

In neuroscience, there's a model of consciousness called the 'workspace' model. The idea is that the various modules in the brain, like the auditory or visual or long-term memory modules normally operate on their own, doing their things, predicting & perceiving what they can; but sometimes something goes wrong: the predictions are suddenly all wrong, or there's unusual & urgent input. The modules panic and emit a summary of the situation over to the single global workspace, where it sits side by side with all the other summaries, and the slow linear prefrontal cortex ponders all the situations & weighs their importance (perhaps issuing some requests to various memories) & sends out orders. In other words, one is only conscious when there is conflict between modules; otherwise, one is unconscious and the modules continue their work. When carrying a dish from the kitchen to the table, one is largely unconscious - one isn't really thinking, one can't remember much, because not much is happening in consciousness. But if the plate is burning hot? Then all of a sudden there is conflict: the arm neurons are frantically trying to execute the 'flinch' reflex, another part is frantically saying don't drop it we're almost there! and the multiple summaries arrive in consciousness, one suddenly 'wakes up' and decides to drop it or not to drop it, and the deed is done.

Why do people ride roller-coasters? Why do they go into haunted houses? They say it makes them feel alive, that it's vivid and unusual, that it's very exciting.. That it wakes them up.

http://www.rifters.com/crawl/?p=791

# _Full Metal Alchemist_

TODO: there's some general essay I could write about FMA, especially the manga versus anime+movie

> What do you think about Mustang using the philosopher's stone entirely to get his vision back?

Kosher. Mustang didn't ask to see the Gate, and that stone would otherwise have been wasted. He didn't merit his punishment.

> About him not taking the seat as the fuhrer?

With the corrupt establishment toppled, there's no longer any compelling reason for him to be fuhrer. Indeed, his personal failings may mean that it's better for him to not be fuhrer. (What would he do?)

> Ed transmuting the literal gate to break the rules?

That wasn't rule-breaking; that was awesome. It was tremendously satisfying.

One of FMA's running themes was the narrowness of those interested in alchemy. They were interested in it, in using it, in getting more of it. Obviously folks like Shou Tucker or Kimbly sold their soul for alchemy, but less obviously, the other alchemists have been corrupted to some degree by it. Even heroes like Izumi or the Elrics transgressed. Consider Mustang; his connection with Hawkeye was alchemy-based, and only after years did the connection blossom. Consider how little time he spent with Hughes, in part due to his alchemy-based position. Mustang didn't learn until Hughes was gone just how much his friends meant.

Similarly, Greed. His epiphany at the end hammers in the lesson about the value of friends. How did he lose *his* friends? By pursuit of alchemy-based methods of immortality.

That is why Ed was the real hero. Because he realized the Truth of FMA: your relationships are what really matter. No alchemist ever escaped the Gate essentially intact before he did. Why? Because it would never even occur to them to give up their alchemy or what they learned at the Gate.

Have you ever heard of a monkey trap made of a hole and a collar of spikes sticking down? The monkey reaches in and grabs the fruit inside, but his fist is too big to pass back out. If only the stupid monkey would let go of the fruit, he could escape. But he won't. And then the hunter comes.

The alchemists are the monkey, alchemy is the fruit, and the Truth is the hunter. The monkeys put the fruit above their lives, because they think they can have it all. Ed doesn't.

(Were there things I disliked? Yes, the whole god thing struck me as strange and ill-thought out. I also disliked the mechanism for alchemy of some sort of Earth energy. I thought that the idea that alchemy was powered by deaths in an alternate Earth to really fit the whole theme of Equivalent Exchange - TANSTAAFL. As it is, alchemy turns out to be a free lunch.)

# Fake explanation of cats

    21:47:59 < gwern> I often think that cat psychology is harder than dog psychology
    21:48:14 < gwern> then I reflect that dogs have co-evolved with us for much longer than cats, and dogs have bigger brains as well
    21:48:15 < cwillu> no, the dominance hierarchy is firmly established
    21:48:25 < gwern> so maybe I only think I understand dogs
    21:49:08 < gwern> perhaps under the evolved tricks like eye-following or pointing-understand lies a psychology as or more alien than cats
    21:49:23 < gwern> *pointing-understanding
    21:49:36 < AngryParsley> cat psychology is that they do whatever the hell they want
    fake explanation ^ <http://lesswrong.com/lw/ip/fake_explanations/>

Cats mimic children: <http://news.discovery.com/animals/cats-humans-pets-relationships-110224.html> <http://www.cell.com/current-biology/retrieve/pii/S0960982209011683>

# Let's nuke Africa

w/r/t existential threats, when is it a better idea to bomb failed nations/continents back to the stone age? <http://lesswrong.com/lw/1qf/the_craigslist_revolution_a_realworld_application/1lud?c=1>

> The absence of rule of law, democratic checks on the military, continual conflict and overall incompetence also increases the chances lab error or misuse of high tech weaponry as technology become more accessible while social, economic and political conditions do not improve.

I just had a fun idea: take this premise, and the demonstrated difficulty of improving Africa, and the idea that the development vs. likeliness-to-screw-everybody-over-with-WMDs curve would be an inverted U, and calculate the point at which it would be better to cut off all aid & begin bombing Africa into (or within) the Stone Age.

> There a high moral cost to beginning bombing Africa.

There is no moral cost by definition; at the point at which we would want to start bombing, the immoral thing is to not bomb. We've bombed many countries for far less than existential threats (arguably, every US bombing campaign back to WWII).

Further, I think you drastically overestimate the chances of homegrown terrorism. Vietnam was long ago. Reports like millions of Iraqi refugees or hundreds of thousands of excess Iraqi deaths merely spark muted partisan arguments about whether the Lancet's statistics are right or not. It's a long way to Tipperary.

> The Global economy would tailspin and the existential risk situation would get a lot worse as a result.

I think you badly overestimate how important Africa is. Even assuming resources cannot be extracted while also bombing the place, Africa isn't that important.

The continental GDP is just 2.7 trillion. Several percent of that is foreign aid ([Economy of Africa](!Wikipedia)) and their exports to the rest of the world are small enough that their balance of payments (with the rest of the world) is negative by billions (http://www.africaneconomicoutlook.org/en/data-statistics/).

Now, if Africa disappeared or was suddenly destroyed, I would expect the global financial markets to drop considerably; but they are so skittish they drop at the fall of a hat. The long-term economic impact wouldn't be so bad outside of commodities like coltan. Certainly not so bad as some grey goo getting loose.

(I'd count things like AIDS as further debits to Africa, but obviously that's a sunk cost as far as this suggestion is concerned.)

# Geneva culinary crimes tribunal

'King Krryllok stated that Crustacistan had submitted a preliminary indictment of Gary Krug, "the butcher of Boston", laying out in detail his systematic genocide of lobsters, shrimp, and others conducted in his Red Lobster franchisee; international law experts predicted that Krug's legal team would challenge the origin of the records under the poisoned tree doctrine, pointing to news reports that said records were obtained via industrial espionage of Red Lobster Inc. When reached for comment, Krug evinced confusion and asked the reporter whether he would like tonight's special on fried scallops'

# Multiple interpretations theory of humor

My theory is that humor is when there is a connection between the joke & punchline which is obvious to the person in retrospect, but not initially.

Hence, a pun is funny because the connection is unpredictable in advance, but clear in retrospect; Eliezer's joke about the motorist and the asylum inmate is funny because we were predicting some other response other than the logical one; similarly for 'why did the duck cross the road? to get to the other side' is not funny to someone who has never heard any of the road jokes, but to someone who has and is thinking of zany explanations, the reversion to normality is unpredicted.

Your theory doesn't work with absurdist humor. There isn't initially 1 valid decoding, much less 2.

Mm. This might work for some proofs - Lewis Carroll, as we all know, was a mathematician - but a proof for something you already believe that is conducted via tedious steps is not humorous by anyone's lights. Proving P/=NP is not funny, but proving 2+2=3 is funny.

'A man walks into a bar and says "Ow."'

> How many surrealists does it take to change a lightbulb? Two. One to hold the giraffe, and one to put the clocks in the bathtub.

Exactly. What are the 2 valid decodings of that? I struggle to come up with just 1 valid decoding involving giraffes and bathtubs; like the duck crossing the road, the joke is the frustration of our attempt to find the connection.

# Mr. T(athagata)

idea: Mr. T as modern Bodhisattva. He remains in the world because he pities da fools trapped in the Wheel of Reincarnation.

# Musical instruments are not about music

To get a rough estimate of how many musical instruments (like the piano) are we can look through Wikipedia's [Category:Musical instruments](!Wikipedia). Lots of non-instruments and notable examples of a kind of instrument, but it makes the numbers pretty clear - there are hundreds of instruments if not thousands, from most cultures, even if we compress the variations.

Suppose one had a well-defined aesthetic preference - a [total ordering](!Wikipedia) (or at least a [partially ordered set](!Wikipedia) with a [greatest element](!Wikipedia)) - so we can speak of an 'ideal instrument' for that person, an instrument which gives them the greatest aesthetic gratification of all known instruments.

If we picked a random instrument for them from our set of thousands of instruments, obviously the odds aren't good we'll pick the ideal one. Thousands to one, after all. If a parent inflicted such a choice on their kid, the kid ought to believe the choice is suboptimal from his aesthetic point of view (with a confidence of >99%). if he cares about the matter, then he should probably go looking as an adult for a better choice.

Depending on how much he cares and how easy it is to 'search' through thousands of instruments, he might search quite a bit.

Strangely, you don't see much of this. Most people seem pretty happy with their current instrument and even music nerds don't spend as much time as one might expect sampling instruments and pondering their merits. How to explain this? [Sunk cost](!Wikipedia) into learning the inferior instrument? Maybe the aesthetic difference between an average and the ideal isn't that great (despite a theremin sounding very different from a synthesizer or a keyboard or a piano, or even violas and violins sounding quite different, and the [revealed preference](!Wikipedia) of antique highly-regarded individual instruments going for hundreds of thousands or millions of dollars to performers)? Or maybe it's.... status.

Maybe people don't search through all manner of rare instruments because musical instruments aren't about aesthetics as much as they are about social [signals](!Wikipedia "Signalling theory") and [status](!Wikipedia "Social status") and prestige. There can only be a few prestigious instruments (perhaps less than 10; surely not as high as 20), after all, and we all hear them quite a bit. By the time a kid hits middle school, he's spent many years watching movies and TV where there's a lot of instrumental background music and he's learned whether he likes piano better than violin or cello.

There's just not many options to think about. If you aspire to WASPy high society, you learn piano; if you aspire to prestige among young people, the guitar or drums. And so on. This is so obvious and ingrained it can be difficult to see; Western society does not, that I know of, have any standard expression like the [Four Arts of the Chinese Scholar](!Wikipedia) (which mandates a scholar know the [guqin](!Wikipedia) and such rules of etiquette as the [Seven Should-not-plays](!Wikipedia) or [Six Avoidances](!Wikipedia)). But nevertheless, the [bongo drums](!Wikipedia) are not prestigious and similarly one can point out the middling status of the [harmonica](!Wikipedia) (which only avoids being low by its use in the blues and jazz). Note that in the stereotype of Asian parents in America forcing their kids to learn instruments, the parents are not choosing oddball instruments you've never heard of (you know, one of the thousands of instruments *not* included in your standard Western-style orchestra), they're choosing ones as familiar as dirt:

> "Let's go back to her crazy list of why her parenting is better. #9: violin or piano, no other instruments. If Chua is so Chinese, and has full executive control over her kids, why does she--and the real Chinese parents out there--make their kids play violin, play Bach and not Chinese music? They'd be happy to educate you on the beauty of Chinese music, I'm sure, but they don't make their kids learn that. Why not?
>
> She wants them learning this because the Western culture deems classical music as high culture, and therefore anyone who can play it is cultured. Someone said Beethoven is great music so they learn that. There is no sense of understanding, it is purely a technical accomplishment. Why Beethoven and not Beethoven's contemporaries? The parents have no idea. Can her kids write new music? Do they want to write music? It's all mechanics. This isn't a slander on Asian musicianship, it is an observation that the parents who push their kids into these instruments are doing it for its significance to other people (e.g. colleges) and not for itself. Why not guitar? Why not painting? Because it doesn't impress admissions counselors. What if the kid shows some interest in drama? Well, then kid can go live with his white friends and see how far he gets in life.
>
> That's why it's in the _WSJ_. The _Journal_ has no place for, "How a [Fender Strat](!Wikipedia) Changed My Life." It wants piano and violin, it wants Chua's college-resume worldview." --["Are Chinese Mothers Superior To American Mothers?"](http://thelastpsychiatrist.com/2011/01/why_chinese_mothers_are_not_su.html), [_The Last Psychiatrist_](http://thelastpsychiatrist.com/)

Or Chua herself:

> "That's one of the reasons I insisted [her two daughters] do classical music. I knew that I couldn't artificially make them feel like poor immigrant kids. ... But I *could* make sure that [daughter #1] and [daughter #2] were deeper and more cultivated than my parents and I were. Classical music was the opposite of decline, the opposite of laziness, vulgarity, and spoiledness. It was a way for my children to achieve something I hadn't. But it was also a tie-in to the high cultural tradition of my ancestors."

It's simple logic that the less popular an instrument, the easier it is to become world-class in it. Standardizing on just a few instrument and turning them into [positional good](!Wikipedia)s also tragically turns them into an arms race:

> "On the whole, discipline makes life easier and better. On the other hand, who the fuck cares about the piano and violin? If all tiger mothers push the piano, say, the winner-take-all race for piano becomes utterly brutal, and the tiger-mothered pianist will likely get less far in the piano race than a bunny-mothered [bassoonist](!Wikipedia). That just seems dumb! Gamble on the [flugelhorn](!Wikipedia)! The Western ethos of hyper-individuation produces less of the sort of hugely inefficient positional pileup (not that there aren't too many guitarists) that comes from herding everybody onto the same rutted status tracks. It also produces less discipline and thus less virtuosity, but a greater variety of excellence by generating the cultural innovation that opens up new fields of endeavor and new status games. It's just way better to be the world's best acrobatic kite-surfer than the third best pianist in Cleveland." --["Amy Chua"](http://www.willwilkinson.net/flybottle/2011/01/20/amy-chua/), [Will Wilkinson](!Wikipedia)

The defense for these practices?

> "There are definitely aspects of my upbringing that I'd like to replicate. I'm never going to be a professional pianist, but the piano has given me confidence that totally shapes my life. I feel that if I work hard enough, I can do anything. I know I can focus on a given task for hours at a time. And on horrible days when I'm lost and a mess, I can say to myself, "I'm good at something that I really, really love." --["Q&A: elves, dirt, and college decisions"](http://tigersophia.blogspot.com/2011/04/q-elves-dirtand-michelangelo.html), Sophia Rubenfeld-Chua

> "The point of learning the piano is NOT about acquiring the skill of playing the piano so that the student can earn a living as a pianist. It is about building the character of the person. Here is the thing about character -- you can't build it by explicitly setting out to build it. Character is not a skill like tying your shoelaces. If it must be put in terms of "skill", character is a "meta-skill" -- a foundational human skill that is necessary to perfect any number of mechanical skills. And the only way to develop this meta-skill is to develop at least one highly sophisticated mechanical skill, such that the student may acquire the meta-skill in the course of building the mechanical skill.
>
> So, once again: the point of learning the piano is NOT about acquiring the skill of playing the piano. As Rubenfeld-Chua put it, it is about acquiring genuine confidence and iron discipline. With such confidence and discipline, she can move on and do anything she wants in her life because there is no task in life in which confidence and discipline hinder success. THIS is the whole point of Tiger Parenting, and the reason why Tiger Parenting is so successful." --["Confucianism and Korea - Part V: What Can Confucianism Do For America?"](http://askakorean.blogspot.com/2011/05/confucianism-and-korea-part-v-what-can.html), The Korean

The cynical questions almost ask themselves. Would Sophia love piano so much if she hadn't had [to practice](!Wikipedia "Cognitive dissonance") [so much](!Wikipedia "Stockholm syndrome")? How unlikely is it that piano would just happen to be the perfect instrument for her? And like the old argument that learning Latin was worthwhile because it sped up subsequent language learning, does the building-character argument actually build character? [Juvenile boot camps](!Wikipedia "Boot camp (correctional)") have generally failed to show any significant improvement in their inmates, and soldiers frequently discuss the difficulty of adapting to civilian life (despite decades of self-discipline)[^rand]. Were we to grant the character-building nature of piano, that raises a further question - don't other instruments build character as well, and so why not learn the flugelhorn and gain *both* benefits - character *and* useful skill building? Why must we all pile into the same high-prestige occupations like being a rock star or actor[^sailer]? This may be good for the tiny subset of "insiders: pianists, concert presenters and pianophiles" who are actually able to notice the differences and value highly small improvements, though even they seem to be a bit jaded and no longer very interested in technical proficiency[^dimeadozen] - but everyone else?

[^dimeadozen]: ["Virtuosos Becoming a Dime a Dozen"](https://www.nytimes.com/2011/08/14/arts/music/yuja-wang-and-kirill-gerstein-lead-a-new-piano-generation.html), Anthony Tommasini, _New York Times_; besides inadvertently making the point that we truly do not need more pianists, Tommasini also adds some fodder to the notion that this is not *just* an East Asian or Asian-American arms race but includes Europe and Russia as well:

    > "...Ms. Wang's virtuosity is stunning. But is that so unusual these days? Not really. That a young pianist has come along who can seemingly play anything, and easily, is not the big deal it would have been a short time ago. The overall level of technical proficiency in instrumental playing, especially on the piano, has increased steadily over time. Many piano teachers, critics and commentators have noted the phenomenon, which is not unlike what happens in sports...Something similar has long been occurring with pianists. And in the last decade or so the growth of technical proficiency has seemed exponential. Yes, Ms. Wang, who will make her New York recital debut at Carnegie Hall in October, can play anything. But in China alone, in recent years, there have been Lang Lang and Yundi Li. Russia has given us Kirill Gerstein, born in 1979, the latest recipient of the distinguished Gilmore Artist Award
    >
    > ...Because so many pianists are so good, many concertgoers have simply come to expect that any soloist playing the Tchaikovsky First Concerto with the New York Philharmonic will be a phenomenal technician. A new level of technical excellence is expected of emerging pianists. I see it not just on the concert circuit but also at conservatories and colleges. In recent years, at recitals and chamber music programs at the Juilliard School and elsewhere, particularly with contemporary-music ensembles, I have repeatedly been struck by the sheer level of instrumental expertise that seems a given. ...The first several decades of the 20th century are considered a golden era by many piano buffs, a time when artistic imagination and musical richness were valued more than technical perfection. There were certainly pianists during that period who had exquisite, impressive technique, like Josef Lhevinne and Rachmaninoff himself. And white-hot virtuosos like the young Vladimir Horowitz wowed the public. But audiences and critics tolerated a lot of playing that would be considered sloppy today. Listen to 1920s and '30s recordings of the pianist Alfred Cortot, immensely respected in his day. He would probably not be admitted to Juilliard now. Despite the refinement and élan in his playing, his recording of Chopin's 24 études from the early 1930s is, by today's standards, littered with clinkers.
    >
    > ...I would place essential artists today like Richard Goode, Mitsuko Uchida and Andras Schiff among the group with all the technique they need. Among younger pianists, this club would include Jonathan Biss, a sensitive, musically scrupulous player; and one of my new favorites, the young Israeli David Greilsammer, who played an inspiring program at the Walter Reade Theater last year in which he made connections among composers from Monteverdi to John Adams, with stops at Rameau, Janacek, Ligeti and more. He may not be a supervirtuoso. But I find his elegant artistry and pianism more gratifying than the hyperexpressive virtuosity of Lang Lang, whose astonishing technique I certainly salute. Besides, the group of play-anything pianists, of which Mr. Lang is a leader, is getting pretty big. Among them you would have to include Garrick Ohlsson, who not only plays with resourceful mastery but seems to play everything, including all the works of Chopin. I would include Leif Ove Andsnes, an artist I revere, who does not call attention to himself but plays with exquisite technique and vibrant musicality. This list goes on. Martha Argerich can be a wild woman at the piano, but who cares? She has stupefying technique and arresting musical ideas. I would add Krystian Zimerman, Marc-André Hamelin and probably Jean-Yves Thibaudet to this roster. There are others, both older and younger pianists....After Mr. Kissin's Liszt Sonata a piano enthusiast sitting near me asked, “Have you ever heard the piece played so magnificently?” I said that the performance was indeed amazing, but that actually, yes, I had heard a comparably magnificent performance on the same stage a few months earlier during a recital by Stephen Hough. Mr. Hough's playing was just as prodigious technically, and I found his conception more engrossing. He reconciled the episodic sections of this teeming work into an awesome entity. Mr. Hough is another pianist who can play anything. Join the club."

But hey, there is *one* benefit to all this futile status signaling. We get a ton of anime music played with classical instruments on YouTube![^touhou]

[^rand]: Mental disorders like post-traumatic stress disorder and suicide, perhaps the ultimate indicator of unhappiness, are increasingly common in former American soldiers thanks to the perpetual War on Terror:

    - From 1999-2004, [70,000 new veterans](http://www.salon.com/news/feature/2005/08/09/vets) received the highest level [PTSD](!Wikipedia) benefits, for 216,000 diagnosed PTSD cases in total; "17 percent of troops returning from duty in Iraq met the strict screening criteria for mental problems such as PTSD" (which is still better than 30% in Vietnam, but worse than 10% in WWII)
    - A [2007 study](http://www.economist.com/node/12792611?story_id=12792611) found a 12% PTSD rate for the Iraq & Afghanistan theaters (>1.8m Americans deployed)
    - [162 active-duty soldiers](https://www.nytimes.com/2011/02/13/us/13drugs.html?_r=2) committed suicide in 2009 (with 101 drug deaths 2006-2009, some of which may be suicides); estimates of PTSD now start at 300,000
    - 2009 and 2010 [active-duty suicides](http://www.congress.org/news/2011/01/24/more_troops_lost_to_suicide) exceeded combat deaths in Iraq and Afghanistan
    - the VA deals with [950 suicide attempts](http://www.rand.org/commentary/2011/05/29/ND.html) as of April 2010, See also the RAND study ["The War Within: Preventing Suicide in the U.S. Military"](http://www.rand.org/pubs/monographs/MG953.html) which covers the 50% rise in suicide rates 2001-2008, and tactics like [Battle buddy](!Wikipedia).

    With all that in mind, it is interesting to consider the positives and negatives of being a [military brat](!Wikipedia "Military brat (US subculture)#Studies of military brats").
[^sailer]: [Steve Sailer](!Wikipedia) [retells one](http://takimag.com/article/the_second_least_glamorous_job_in_showbiz) of many jokes expressing this observation:

    > “I can't get a date, Doc,” the new patient griped to his psychiatrist. “See, I sweep up the circus elephants' droppings and can never wash the stench off me.”
    >
    > “Perhaps you should get a different job.”
    >
    > “What, and quit show business?”
[^touhou]: This is a general assertion that is fairly hard to prove, but an example may be suggestive. The [Touhou](!Wikipedia "Touhou Project") doujinshi-game phenomenon has a [fair amount of music](http://touhou.wikia.com/wiki/Category:Music), but to get a sense of the true scale, we can look at some numbers. From the talk ["Riding on Fans' Energy: Touhou, Fan Culture, and Grassroot Entertainment"](http://cardcaptor.moekaku.com/?p=112) ([Barcamp](!Wikipedia) Bangkok 2 on August 31, 2008):

    > "Touhou is [ZUN](!Wikipedia "Team Shanghai Alice#Member")'s work as much as it is a gigantic repertoire of fan-made manga, games, music, and video clips. I estimate that there are roughly at least three thousands short manga, five hundred music rearrangement albums, and one hundred derivative games created since 2003. These works are traded mainly in conventions dedicated to them, and some commercial firms are starting to capitalize on their popularity. Doujinshi shops like [Tora no Ana](!Wikipedia "Comic Toranoana") and [Mandarake](!Wikipedia) have shelves dedicate to Touhou comics. And Amazon.co.jp are carrying CDs of arranged/sampled Touhou music (but not ZUN's originals). More and more people are attracted to the franchise because its diverse derivative works provide a variety of entry points for potential fans. In fact, Touhou's popularity skyrocketed when it became one of the killer content of [Nico Nico Douga](!Wikipedia), a Japanese equivalent of YouTube launched one year and a half earlier. There, Touhou content spread like wild fire and gave rise to many recurring memes and tens of thousands of mashup videos. To give a sense of how popular Touhou is in Nico Nico Douga, 18 of 100 most viewed videos are Touhou-related, and the best Touhou video ranks the 6th. [[5]](http://cardcaptor.moekaku.com/?p=112#touhou-popularity-in-nico-nico-douga)"

    For a recent estimate, we can turn to [TV Tropes](!Wikipedia)'s [article](http://tvtropes.org/pmwiki/pmwiki.php/CrowningMusic/TouhouProject) on Touhou music:

    > "The Touhou Project really gets a lot of great pieces of music for [the music] being [originally] made up by a single guy with a synthesizer. To put the sheer number of remix CDs in perspective, there is a torrent with over 870.4 gigabytes of over 3000 Touhou remixes, and that only includes the ones that the (English-speaking) maintainers of the torrent have added."

    (Personally, I enjoy the orchestral pieces like the [WAVE](http://www.circle-wave.net/) group's [Luna Forest (第七楽章)](http://www.youtube.com/watch?v=xhDNC12hWKc).)

# Lip reading website

As far as I can tell, there is no free resource for learning how to [lip read](!Wikipedia), much less free online resources.

Learning to lip read is basically:

1. watch a video with obscured audio
2. guess what you think they said
3. be corrected
4. go to #1

This is eminently doable as a website: YouTube for hosting videos, [Amazon Mechanical Turk](!Wikipedia) or similar services for generating Free videos, and perhaps a [SRS algorithm](!Wikipedia "Spaced repetition") for scheduling periodic reviews videos of particular words or sentences.

Lip reading is useful to know. There are roughly [28 million](http://www.audiologyonline.com/articles/pf_article_detail.asp?article_id=1204) people in the US with hearing issues and as the [Baby Boomers](!Wikipedia) age and lose hearing, many will want to learn; estimates of Baby Boomers who will have any degree of hearing loss range from 20-60%. See:

- <http://www.hearform.com/articles/hearing_loss_baby_boomers.htm>
- <http://webcitation.org/5nn3LKFfP>
- <http://www.healthyhearing.com/articles/45443-hearing-loss-baby-boomers>
- <http://archives.cnn.com/2000/HEALTH/aging/04/19/hearing.loss.wmd/index.html>

For older Americans, the rate is [63%](http://www.nytimes.com/2011/03/08/health/research/08aging.html).

Nor is the loss limited to Baby Boomers; a report in the August 2010 issue of the American Medical Association estimated that over the 15 years from 1995 to 2010, teen hearing loss rate increased 30%. (Cited in _The Futurist_ November 2010.)

The small industry of lip reading and the international scattering of lip reading classes shows that people will pay hundreds of dollars and go places to learn it.

## Costs

Optimistically, one-time >100$ for content, 20$ / month then on, and a substantial time investment in putting together a site and a process for acquiring or creating video.

### Technical
#### Hosting

Assuming videos are hosted on YouTube or [Amazon S3](!Wikipedia), the website would require extremely little bandwidth and accommodate >1000 users at ~20$ a month:

Assume a webpage requires 100KB to be loaded (very pessimistic), and that a user spends 1 hour a day using the website (30 hours a month), going to a new page every minute. That user will use $100\text{KB} \times (30 \times 60)$, or 180000KB, or 180MB of bandwidth. [Linode's](http://www.linode.com/) cheapest offering at $20/month pays for 200GB of bandwidth; $\frac{200\text{GB}}{180\text{MB}}$ = 1112 users. A domain name costs ~$10 a year, or ~$1 a month.

More reasonable would be assuming 10KB per pageload, and 10 hours a month, cutting the per-user bandwidth down to $10 \times 10 \times 60$, or 6MB, and assuming fewer than 1000 users; then hosting could be even cheaper. [DreamHost](http://www.dreamhost.com/hosting.html) is known for screwing over its more-demanding customers, but should be reliable enough here; their hosting is $9 a month.

#### Coding

Obviously a site custom-made for lip reading & very user-friendly doesn't exist. I'd have to code one or reuse some framework, though offhand I don't know of any really suited for the task. It'd be a big coding task - at least dozens of hours to learn the specific technologies and build a prototype. But then, I can't really count my own time as a cost - I'd just spend the time reading elsewise.

### Marketing

Unknown. These sorts of sites seem to do best with word of mouth marketing, so who knows? Maybe just time.

### Content

The content is the wildcard. There are a couple possible sources:

- There's a cottage industry of books and occasional CDs/DVDs, whose copyright obviously would be far too expensive to purchase.
- Hiring professionals to make lip movements also is obviously right out. To make it worth their while and to get at least 10 hours of material would take thousands of dollars.
- Online freelancing sites. I have a theory that one doesn't *want* professionals because one intends to use lip reading in real life, to read the lips of the 'amateurs' one interacts with.
     - I mentioned Mechanical Turk, but that may not be appropriate; many Turkers do not have cameras or webcams, and it may not be doable to ask them to submit videos through Amazon, but Turkers could definitely be used to verify that the person in a clip is saying the things they are supposed to say. (This would cost ~10¢ per review, and usually one double-checks with multiple Turkers, so 20¢ a clip.)
    - Other freelancing sites like guru.com list video/photo people working for $20-40 an hour. I figure that means that amateurs in both department will be no more than half that, $10/hour. 10 hours of content then would be ~$100.

## Revenue

Ads, obviously. A competitor would be lipreading.com; so that's a reasonable starting point. With zero effort at doing anything other than selling a DVD, some estimates of its ad revenue are [44¢](http://www.cubestat.com/www.lipreading.com) to [$2.22](http://www.websiteoutlook.com/www.lipreading.com) a day. At hosting costs of $21 a month or <75¢ a day, the site could at least pay its on-going expenses.

## Links

Random links that may be of interest:

- <http://deafness.about.com/cs/communication/a/lipreading.htm>
- <http://www.hearinglossweb.com/Issues/OralCommunications/Strategies/sphrd/allen.htm>
- <http://www.usnews.com/mobile/articles_mobile/computers-might-make-learning-lip-reading-easier/index.html>
- <http://www.lipread.com.au/Products.html>
- <http://www.bellaonline.com/subjects/2664.asp>
- <http://www.deafcando.org.au/index.php?/deafcando/services/C81/>
- <http://www.adcohearing.com/bvs_hear_loss.html>

# Venusian Revolution

> "Venus is a great example. It does pretty well in the equation, and actually gets a value of about one and a half quadrillion dollars if you tweak its reflectivity a bit to factor in its bright clouds. This echoes what unfolded for Venus in the first half of the 20th century, when astronomers saw these bright clouds and thought they were water clouds, and that it was really humid and warm on the surface. It gave rise to this idea in the 1930s that Venus was a jungle planet. So you put this in the formula, and it has an explosive valuation. Then you'd show up and face the reality of lead melting on the surface beneath sulfuric-acid clouds, and everyone would want their money back!
>
> If Venus is valued using its actual surface temperature, it's like 10^-12^ of a single cent. @home.com was valued on the order of a billion dollars for its market cap, and the stock is now literally worth zero. Venus is unfortunately the @home.com of planets.
>
> It's tragic, amazing, and extraordinary, to think that there was a small window, in 1956, 1957, when it wasn't clear yet that Venus was a strong microwave emitter and thus was inhospitably hot.
>
> The scientific opinion was already going against Venus having a clement surface, but in those years you could still credibly imagine that Venus was a habitable environment, and you had authors like Ray Bradbury writing great stories about it. At the same time, the ability to travel to Venus was completely within our grasp in a way that, shockingly, it may not be now. Think what would have happened, how history would've changed, if Venus had been a quadrillion-dollar world, we'd have had a virgin planet sitting right next door. Things would have unfolded in an extremely different way. we'd be living in a very different time."

    --Greg Laughlin, interviewed in ["Cosmic Commodities: How much is a new planet worth? "](http://www.boingboing.net/2011/02/03/cosmic-commodities-h.html)

Sounds like a good alternate history novel. The space race heats up in the 1950s, with a new planet at stake. Of course, along the lines of [Peter Thiel's reasoning](http://www.hoover.org/publications/policy-review/article/5646) about France & John Law & the Louisiana Territory, the 'winner' of such a race would probably suffer the [winner's curse](!Wikipedia). (Don't go mine for gold yourself; sell pick-axes to the miners instead.)

# Pseudonymity

'Gwern Branwen' is the pseudonym I use to traffick online; I was long paranoid and sought to cleanly separate my online and offline life. (With the exception of commercial transactions - I didn't particularly mind if Amazon.com was able to link my credit card under my real name with my email account.) This was a good idea because I picked up the occasional online enemy who could annoy me.

As Maru Dubshinki, I made an enemy of Daniel Brandt; and more recently, there was an attempted harassment of me in real life, /b/-style, over Neil Gaiman's [Scientology connections](!Wikipedia "Neil Gaiman#Early life") which failed miserably when their investigation dead-ended at calling up [RIT](!Wikipedia) and asking whether a Gwern Branwen happened to work there. Which of course he did not. My biggest mistake, when I still cared about solid pseudonymity, was occasionally joining the [#wikipedia IRC channel](irc://irc.freenode.net#wikipedia) without my [IRC cloak](!Wikipedia) and thereby exposing my IP address; Daniel Brandt was able to narrow me to someone living in [Suffolk County, New York](!Wikipedia).

So, I had always expected a break in my pseudonymity to come from the online direction. I hadn't expected it to come from the other direction. Then one day in 2008 or 2009, it did...

I was idling in #wikipedia, discussing Wikipedia matters, as one does on the weekend, when [Andrew Garrett](!Wikipedia "User:Werdna") and [Jennifer Boriss](!Wikipedia "User:FlyingToaster") ([homepage](http://www.jennyboriss.com/)) happened to also be hanging out with my older sister at Carnegie-Mellon; they fell to discussing Wikipedia and she mentioned that her younger brother did an awful lot of Wikipedia editing (as I [do](Links#wikis)), and Boriss inquired as to who I was when I was not at home. It was some strange nickname she couldn't remember. But she *did* remember that I frequently edited Japanese literature articles and the nick started with 'g' or something. (I'm particularly proud of [Fujiwara no Teika](!Wikipedia), incidentally.)

Well, she says, I don't know many Japanese literature editors; but I do chat with this guy named Gwern on IRC who does a lot of that sort of thing. That's it!, my sister says (as she casually destroys my pseudonymity), the odd nickname was 'Gwern'. They both have a good laugh about what a small world it is, and then they gs on IRC and knock me for a loop by 'guessing' personal details until they finally reveals that my 'hot sister' is there providing information. And of course since my sister now works in San Francisco, and Garrett is a PHP developer contracted to the Wikimedia Foundation & Boriss a developer for the Mozilla Foundation (both headquartered in or near SF), she or he occasionally visits & stays with my sister and I have to hear about it online from them. Oy vey.

# Efficient natural language

A single English character can be expressed (in [ASCII](!Wikipedia)) using a byte, or ignoring the wasted high-order bit, a full 7 bits. But English is pretty predictable, and isn't using those 7 bits to good effect. [Claude Shannon](!Wikipedia) found that each character was carrying more like 1 bit of unguessable information[^shannon]; Hamid Moradi found 1.62-2.28 bits on various books[^moradi], and Cover came up with 1.3 bits[^cover]. In practice, existing algorithms can make it down to just 2 bits to represent a character, and theory suggests the true entropy was around 0.8 bits per character.[^grassberger] (This, incidentally implies that the highest bandwidth human speech can attain is around 55 bits per second.[^rapping]) Languages can vary in how much they convey in a single 'word' - ancient Egyptian conveying ~7 bits per word and modern Finnish around 10.4[^plos] (and word ordering adding another at least 3 bits over most languages); but we'll ignore those complications.

[^shannon]: Claude E. Shannon, ["Prediction and entropy of printed English"](http://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf), _Bell Systems Technical Journal_, pp. 50-64, Jan. 1951
[^cover]: T. M. Cover, ["A convergent gambling estimate of the entropy of English"](http://www.stanford.edu/~cover/papers/transIT/0413cove.pdf), _IEEE Trans. Information Theory_, Volume IT-24, no. 4, pp. 413-421, 1978
[^moradi]: H. Moradi, ["Entropy of English text: Experiments with humans and a machine learning system based on rough sets"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.5610&rep=rep1&type=pdf), _Information Sciences, An International Journal_ 104 (1998), 31-47
[^grassberger]: Peter Grassberger, ["Data Compression and Entropy Estimates by Non-sequential Recursive Pair Substitution"](http://arxiv.org/abs/physics/0207023) (2002)
[^rapping]: Rapper Ricky Brown apparently set a rapping speed record in 2005 with ["723 syllables in 51.27 seconds"](http://sparkplugged.net/2007/10/outsider-speed-rap-extraordinaire/), which is 14.1 syllables a second; if we assume that a syllable is 3 characters on average, and go with an estimate of 1.3 bits per character, then the bits per second (b/s) is $14.1 \times 3 \times 1.3$, or 55 b/s. This is something of a lower bound; Korean rapper [Outsider](!Wikipedia "Outsider (rapper)") claims [17](http://www.arirang.co.kr/TV2/Star_Focus.asp?code=Ki3&F_KEY=612) syllables, which would be 66 b/s.
[^plos]: See ["Universal Entropy of Word Ordering Across Linguistic Families"](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0019875), Montemurro 2011

Whatever the true entropy, it's clear existing English spelling is pretty wasteful. How many characters could we get away with? We could ask, how many bits does it take to uniquely specify 1 out of, say, 100,000 words?  Well, _n_ bits can uniquely specify 2^_n_^ items; we want at least 100,000 items covered by our bits, and as it happens, 2^17^ is 131072, which gives us some room to spare. (2^16^ only gives us 65536, which would be enough for a pidgin or something.) We already pointed out that a character can be represented by 7 bits (in ASCII), so each character accounts for 7 of those 17 bits. 7+7+7 > 17, so 3 characters. In this encoding, one of our 100,000 words would look like 'AxC' (and we'd have 30,000 unused triplets to spare). That's not so bad.

But as has often been [pointed out](http://www.zompist.com/kitlong.html#howmany), one of the advantages of our verbose system which can take as many as 9 characters to express a word like 'advantage' is that the waste also lets us understand partial messages. The example given is a disemvoweled sentence: 'y cn ndrstnd Nglsh txt vn wtht th vwls'. Word lengths themselves correspond roughly to frequency of use^[Which would be a sort of [Huffman encoding](!Wikipedia); see also ["Entropy, and Short Codes"](http://lesswrong.com/lw/o1/entropy_and_short_codes/).] or average information content.^[["Word lengths are optimized for efficient communication"](http://m.pnas.org/content/108/9/3526.short): "We demonstrate a substantial improvement on one of the most celebrated empirical laws in the study of language, [Zipf's](!Wikipedia "Zipf's law") 75-y-old theory that word length is primarily determined by frequency of use. In accord with rational theories of communication, we show across 10 languages that average information content is a much better predictor of word length than frequency. This indicates that human lexicons are efficiently structured for communication by taking into account interword statistical dependencies. Lexical systems result from an optimization of communicative pressures, coding meanings efficiently given the complex statistics of natural language use."]

The answer given when anyone points out that a compressed file can be turned to nonsense by a single error is that errors aren't that common, and the 'natural' redundancy is *very* inefficient in correcting for errors[^vowel], and further, while there are some reasons to expect languages to have evolved towards efficiency, we have at least 2 arguments that they may yet be very inefficient:

1. natural languages differ dramatically in almost every way, as evidence by the difficulty Chomskyians have in finding the [deep structure](!Wikipedia) of language; for example, average word length differs considerably from language to language. (Compare German and English; they are closely related, yet one is shorter.)
2. speakers may prefer a concise short language with powerful error-detecting and correction, since speaking is so tiring and metabolically costly; but listeners would prefer not to have to think hard and prefer that the speaker do all the work for them, and would thus prefer a less concise language with less powerful error-detection and correction[^cancho]

[^cancho]: ["Least effort and the origins of scaling in human language"](http://www.pnas.org/content/100/3/788.abstract?sid=cc7fae18-87c9-4b67-863a-4195bb47c1d1), Cancho 2002. From the abstract:

    > "...In this article, the early hypothesis of Zipf of a principle of least effort for explaining the law is shown to be sound. Simultaneous minimization in the effort of both hearer and speaker is formalized with a simple optimization process operating on a binary matrix of signal–object associations. Zipf's law is found in the transition between referentially useless systems and indexical reference systems. Our finding strongly suggests that Zipf's law is a hallmark of symbolic reference and not a meaningless feature. The implications for the evolution of language are discussed..."

One interesting natural experiment in binary encoding of languages is the [Kele language](!Wikipedia "Kele language (Congo)"); its high and low tones add 1 bit to each syllable, and when the tones are translated to drumbeats, it takes about 8:1 repetition:

> "Kele is a tonal language with two sharply distinct tones. Each syllable is either low or high. The drum language is spoken by a pair of drums with the same two tones. Each Kele word is spoken by the drums as a sequence of low and high beats. In passing from human Kele to drum language, all the information contained in vowels and consonants is lost...in a tonal language like Kele, some information is carried in the tones and survives the transition from human speaker to drums. The fraction of information that survives in a drum word is small, and the words spoken by the drums are correspondingly ambiguous. A single sequence of tones may have hundreds of meanings depending on the missing vowels and consonants. The drum language must resolve the ambiguity of the individual words by adding more words. When enough redundant words are added, the meaning of the message becomes unique.
>
> ...She [his wife] sent him a message in drum language...the message needed to be expressed with redundant and repeated phrases: “White man spirit in forest come come to house of shingles high up above of white man spirit in forest. Woman with yam awaits. Come come.” Carrington heard the message and came home. On the average, about eight words of drum language were needed to transmit one word of human language unambiguously. Western mathematicians would say that about one eighth of the information in the human Kele language belongs to the tones that are transmitted by the drum language."^[["How We Know"](http://www.nybooks.com/articles/archives/2011/mar/10/how-we-know/?pagination=false), by [Freeman Dyson](!Wikipedia) in _[The New York Review of Books](!Wikipedia)_ (review of James Gleick's _The Information: A History, a Theory, a Flood_)]

With a good [FEC](!Wikipedia "Forward error correction"), you can compress and eat your cake too. Exactly how much error we can detect or correct is given by the [Shannon limit](!Wikipedia):

[^vowel]: If we argue that vowels are serving a useful purpose, then there's a problem. There are only 3 vowels and some semi-vowels, so we have at the very start given up at least 20 letters - tons of possibilities. To make a business analogy, you can't burn 90% of your revenue on booze & parties, and make it up on volume. Even the most trivial error-correction is better than vowels. For example, the last letter of every word could specify how many letters there were and what fraction are vowels; 'a' means there was 1 letter and it was a vowel, 'A' means 1 consonant, 'b' means 2 vowels, 'B' means 2 consonants', 'c' means 1 vowel & 1 consonant (in that order), 'C' means the reverse, etc. So if you see 'John looked _tc Julian', the trailing 'c' implies the missing letter is a vowel, which could only be 'a'.

    This point may be clearer if we look at systems of writing. Ancient Hebrew, for example, was an [abjad](!Wikipedia) script, with vowel-indications (like the [niqqud](!Wikipedia)) coming much later. Ancient Hebrew is also a dead language, no longer spoken in the vernacular by its descendants until the [Revival of the Hebrew language](!Wikipedia) as [Modern Hebrew](!Wikipedia), so oral traditions would not help much. But nevertheless, the Bible is still very well-understood, and the lack of vowels rarely an issue; even the complete absence of modern punctuation didn't cause very many problems. The examples I know of are striking for their unimportance - the exact pronunciation of the [Tetragrammaton](!Wikipedia) or whether the [thief crucified](!Wikipedia "Saint Dismas#Today... in paradise") with Jesus immediately went to heaven.

> $\frac{\text{channelCapacity}}{1 - (-(\text{mistakeRate} \times log_2(\text{mistakeRate}) + (1 - \text{mistakeRate}) \times log_2(1 - \text{mistakeRate})))}$

If we suppose that each word is 3 characters long, and we get 1 error every 2 words on average, our channel capacity is 6 characters' of bits (or 7*6, or 42), and our mistake rate 1/6 of the characters (or 7/42), substituting in we get:

> $\frac{42}{1 - (-(\frac{1}{6} \times log_2(\frac{1}{6}) + (1 + \frac{1}{6}) \times log_2(1 - \frac{1}{6})))}$

Or in Haskell, we evaluate (using [logBase](!Hoogle) because [log](!Hoogle) is the natural logarithm, not the binary logarithm used in information theory):

~~~~{.haskell}
42 / 1 - (-(1/6 * logBase 2 (1/6) + (1 - 1/6) * logBase 2 (1 - 1/6)))
~~~~

Which evaluates to ~41. In other words, we started with 42 bits of possibly corrupted information, assumed a certain error rate, and asked how much could we communicate given that error rate; the difference is whatever we had to spend on ECC - 1 bit. Try comparing that to a vowel-scheme. The vowel would not guarantee detection or correction (you may be able to decade 'he st' as 'he sat', but can you decode 'he at' correctly?), and even worse, vowels demand an entire character, a single block of 7/8 bits, and can't be subtly spread over all the characters. So if our 2 words had one vowel, we just blew 7 bits of information on that and that alone, and if there were more than 1 vowel...

Of course, the Shannon limit is the theoretical ideal and requires complex solutions humans couldn't mentally calculate on the fly. In reality, we would have to use something much simpler and hence couldn't get away with devoting just 1 bit to the FEC. But hopefully it demonstrates that vowels are a really atrocious form of error-correction. What would be a good compromise between humanly possible simplicity and inefficiency (compared to the Shannon limit)? I don't know.

[Richard Hamming](!Wikipedia), who invented much of the early error-correcting codes, once devised such a scheme for IBM (similar to the [ISBN check-digit](!Wikipedia "International Standard Book Number#Check digits")); number letters or characters from 1 to 37, and add them all up modulo 37, which is the new prefix to the word. This checksum handles what Hamming considered the most common human errors like repeating or swapping digits.[^hamming]

[^hamming]: from "Coding Theory II" in _The Art of Doing Science and Engineering_, Richard W. Hamming 1997:

    > "...I was once asked by AT&T how to code things when humans were using an alphabet of 26 letter, ten decimal digits, plus a “space”. This is typical of inventory naming, parts naming, and many other naming of things, including the naming of buildings. I knew from telephone dialing error data, as well as long experience in hand computing, humans have a strong tendency to interchange adjacent digits, a 67 is apt to become a 76, as well as change isolated ones, (usually doubling the wrong digit, for example a 556 is likely to emerge as 566). Thus single error detecting is not enough...Ed Gilbert, suggested a weighted code. In particular he suggested assigning the numbers (values) 0, 1, 2, ..., 36 to the symbols 0,1,..., 9, A, B, ..., Z, space.
    >
    > ...To encode a message of n symbols leave the first symbol, _k_=1, blank and whatever the remainder is, which is less than 37, subtract it from 37 and use the corresponding symbol as a check symbol, which is to be put in the first position. Thus the total message, with the check symbol in the first position, will have a check sum of exactly 0. When you examine the interchange of any two different symbols, as well as the change of any single symbol, you see it will destroy the weighted parity check, modulo 37 (provided the two interchanged symbols are not exactly 37 symbols apart!). Without going into the details, it is essential the modulus be a prime number, which 37 is.
    >
    > ...If you were to use this encoding, for example, for inventory parts names, then the first time a wrong part name came to a computer, say at transmission time, if not before (perhaps at order preparation time), the error will be caught; you will not have to wait until the order gets to supply headquarters to be later told that there is no such part or else they have sent the wrong part! Before it leaves your location it will be caught and hence is quite easily corrected at that time. Trivial? Yes! Effective against human errors (as contrasted with the earlier white noise), yes!"

# Charitable supercomputing

[Folding@home](!Wikipedia) boasts of being the largest/most powerful distributed computing project in the world with >5 petaflops of capacity, focused on the [NP-hard](http://www.springerlink.com/content/n111140117672r08/) problem of [protein folding](!Wikipedia). It is powered by volunteers running its client on their computers, or more specifically, their [GPUs](!Wikipedia) and [PS3s](!Wikipedia). (Their architectures are more specialized than normal CPUs and, if a problem happens to match their architecture, can run an order of magnitude or two faster than a CPU would.)

The researchers solve their problems, the volunteers know their idle computing capacity is going to good use - everyone wins. Right?

Where is this free lunch coming from? What does it cost the volunteers to run the clients? Electricity. Lots of electricity.

Wikipedia has [done the legwork](!Wikipedia "Folding@home#Estimated energy consumption") on how much electricity consumption Folding@home causes. Each petaflop is ~3 megawatts, so it is currently using ~15 megawatts every second and has for a while. To put it another way, every hour Folding@home uses up 15 megawatt-hours.

## Sins of commission

Electricity doesn't come from nowhere. If we are to do even the most simplistic cost-benefit analysis, we can't simply assume the cost is 15 megawatts conjured out of nowhere or that the electricity would have been consumed anyway. What a silly defense that would be - what are the power companies doing, generating a fixed amount of electricity and if the Folding@homers shut down, the plants dump their electricity into the air? *Of course* the marginal demand from Folding@home causes the generation of megawatt-hours that would not otherwise have been generated. (Oh, but Folding@home users are so altruistic that they engage in conservation to offset their computer! Yeah, right.) And it doesn't matter what other wastes of electricity may be going on - those wastes stand condemned as well ("two wrongs don't make a right"). No more distractions or excuses: what are the costs of that cost?

The most obvious cost is *air pollution*. It is major enough that we don't even need to consider any other costs, because [air pollution kills](http://nextbigfuture.com/2011/03/global-health-risks.html). The editors of Next Big Future have listed a number of interesting statistics from the WHO and other studies of electricity generation and the deadliness of air pollution.

For example, half the world's electricity production is done by that filthiest and most dangerous of sources, coal. (In comparison, nuclear power is a stroll in the park. French users can run Folding@home with 75% cleaner conscience that the rest of us.) We will - generously - pretend that everyone contributing to Folding@home is located in the US, with its relatively clean coal plants. Nevertheless, each terawatt-hour of coal kills [15 people](http://nextbigfuture.com/2011/03/deaths-per-twh-by-energy-source.html). How often does Folding@home burn through a terawatt? Well, 1000 megawatt-hours is one gigawatt-hour, and 1000 gigawatt-hours is one terawatt-hour, and $1000 \times 1000 = 1,000,000$, so how fast does Folding@home burn through 1 million megawatt-hours? Each year, Folding@home uses $15 \times 24 \times 365.25 = 131,490$ megawatt-hours, or 13.15% of a terawatt-hour. Each terawatt-hour means, remember, 15 dead people; in our grim calculus, what is 13.15% of 15 dead people? 1.8 dead people.

So if the power is entirely derived from coal, Folding@home kills 2 people a year.

What if the power is from an oil plant? That's worse! The listed deaths per terawatt-hour for oil is 36 dead people, for an annual death toll of 4.4 people. Hey, it could be much worse: if Folding@home had been invented and popularized in China, with a terawatt-hour death toll of 278, that'd be 34 deaths every year.

With coal and oil out of the way, we can look at the minority fuels which make up a small slice of the US power supply. Biofuel is pretty bad with a death toll of 12/TWh; hydro isn't too bad at 1.4/TWh; solar, wind, and nuclear power have <1/TWh death tolls. But of course, we don't live in an environmental fantasy where all our power is generated by those cleaner sources, and even if we did, that wouldn't help the people in the past who have already killed by Folding@home's pollution.

The actual power mix of the USA in 2009 was 45% coal, 24% natural gas, 20% nuclear, and 7% hydro, so balancing our numbers that gives us 1.01 annual deaths for a USA power mix. Phew! Only one dead person. Doesn't that make you feel better?

## Sins of omission

We already saw how much electricity Folding@home consumes: 15 megawatt-hours. But how much does each megawatt-hour cost? The [EIA](http://www.eia.gov/cneaf/electricity/epm/epm_sum.html) says the average US rate for 1 kilowatt-hour in November 2010 was $0.0962. A megawatt-hour is 1000 kilowatt-hours, so 1 megawatt-hour is $1000 \times 0.0962$, or $96.2, so $15 \times 96.2$ = $1443/hr. And its annual bill is $1443 \times 24 \times 365.25$, or ~12,650,000 dollars per year.

$12.65 million is a lot of money. What could we do with that? Meta-charity [Givewell](!Wikipedia) [estimates](http://www.givewell.org/giving101) that <$1000 could save one life; another source says ["Cost‐effectiveness estimates per death‐averted are $64‐294 for a range of countries"](http://www.copenhagenconsensus.com/Admin/Public/DWSDownload.aspx?File=%2fFiles%2fFiler%2fCCC%2fBPP_Micronutrient_VitaminA_Zinc.pdf)^[A non-nutrient-based approach would be midwife training; ["Even a small pilot project costing only $20,244 saved the lives of 97 infants, the authors estimated, meaning that it cost just $208 per life saved."](http://www.nytimes.com/2011/05/10/health/10global.html)]. (One [modest proposal](http://www.raikoth.net/deadchild.html) is to use this $1000 figure as the base unit of a new coinage: the DC or 'dead child'; it has the merit over the dollar of possibly ingraining an understanding of [opportunity cost](!Wikipedia)s.) And these interventions are the kind of things that can absorb a lot of money. (There are a lot of people out there who could use some help.)

If <$1000 will buy 1 life, then $12.65m would buy ~12,650 lives. Quite a few, that. One wonders whether Folding@home was the best form of charity the 300k or so volunteers could have chosen to engage in.

Maybe they would have done better to donate a few dollars to a regular charity, and not run up their electricity bill. One might wonder, though, about the case where one isn't paying for one's electricity. So,either you are paying for all of your electrical bill or you're not:

- If you are paying for all of it, then yes, you can donate your electrical costs! Just don't run Folding@Home and send Oxfam a Paypal donation at the end of the year.
- If you are not paying for all of it, if someone is sharing the bill or footing the bill entirely, then donating directly may harm your pocketbook, yes. But in such a situation, does it really still make sense to force the other to pay for all the electricity you are using?

    The overall economics are bad per the original note, it's an inefficient way to turn someone else's money into charity. What right do you have to burn the electricity like there's no tomorrow, for that matter? If you weren't going to use a year's worth of electricity, then whomever is paying for your electricity is poorer by that $10 as surely as if you had pick-pocketed him of $10; few would agree that you are Robin Hood who may steal from the rich and give to the charitable. He accepted that risk when he gave you access to his electricity and deserves however you can contrive to screw him over? What an attitude!

Other points can be dealt with similarly. Perhaps one worries that overhead on a small donation of $10 will eliminate the value. But the overhead on financial transactions is usually only a few percent, and the difference between Folding@home and the best charities is a difference of far more than a few percent. A dollar is a dollar, no matter where it comes from. If you and 99 other people each donate $100 to 100 charities, then it's the same as if each person donated $10,000 to just 1 charity. The only difference is whatever overhead there might be; and even if we say that our Folding@Home contributors lose 50% to overhead and the charities wind up getting only $6 million in their bank accounts to use, that's still thousands more lives saved than by running Folding@Home and wasting the same amount of money![^lw]

[^lw]: The marginal effectiveness of the best charities is huge; the best charities do orders of magnitude more good than mediocre or bad charities. A thought-example from ["Effective Charity"](http://lesswrong.com/lw/37f/efficient_charity/)

    > "A hypothetical charity running programs [like VillageReach's](http://www.givewell.org/international/top-charities/villagereach#Pastcosteffectivenesspilotprogram) which embezzled 95% of its budget and had correspondingly greatly reduced cost-effectiveness would _still_ be doing far more good per dollar than the Make-A-Wish Foundation or the least effective developing world charities do. This example makes it clear how profoundly useless the overhead ratio is for assessing the relative quality of a charity."

    This holds true for less mortal charities; [Nicholas Kristof](https://www.nytimes.com/2011/05/19/opinion/19kristof.html?_r=2&hp) cites an example with *2* orders of magnitude difference:

    > "In much of the developing world, most kids have intestinal worms, leaving them sick, anemic and more likely to miss school. Deworming is very cheap (a pill costing a few pennies), and, in [the experiment he did with Edward Miguel](http://www.economics.harvard.edu/faculty/kremer/files/Worms_I_Econometrica_2003.pdf), it resulted in 25 percent less absenteeism. Even years later, the kids who had been randomly chosen to be dewormed were earning more money than other kids. Kremer estimates that the cost of keeping a kid in school for an additional year by building schools or by subsidizing school uniforms is more than $100, while by deworming kids, the cost drops to $3.50. (In a pinch, kids can usually go to 'school' in a church or mosque without a uniform.)"

## Benefits

But hey, perhaps it's done good research that will save even more lives. Biology, hell yeah!

Wikipedia has a [partial list](!Wikipedia "Folding@home#Results") of 75 papers published drawing in some way on Folding@home. That is an average of 7.5 papers per year. The skeptic will notice that not a few (especially early papers, naturally) seem more concerned with Folding@home _per se_ than with actual new results generated by it, and that project lead Vijay Pande seems to be author or co-author on almost all of the papers, which doesn't indicate a large research community around the large investment of Folding@home. None of them seem important, and the number of publications seems to have peaked back in 2005-2006. The few [actual compounds](http://folding.stanford.edu/English/FAQ-Diseases) seem stalled in their test tubes. And a reminder, the wasted money amounts to many thousands of lives; for these sort of stakes, one would hope that one had *good* evidence, not mere possibility. But let's lower standards and ask for ordinary evidence. What reason do we have to think Folding@Home has potential to save millions of lives? It has been operating for nearly 11 years. 11! And nothing that has yet saved a single life. (Readers are invited to provide a counter-example.) At what point do we stop talking about its potential to save millions of lives and about the possibility of [teapots in orbit](!Wikipedia "Russell's teapot" around Mercury? 'Basic research' is great and all, but at some point enough is enough.

As I am not a biologist nor omniscient, I can't say for *sure* that the Folding@home work wasn't useful, and I certainly couldn't say it looks pretty worthless. But I feel much more comfortable asserting that the $12.65m could have been better spent on saving those 12,000 people (and the 12,000 people the year before, and the 12,000 the year before that, and...).

## Doing it right: Rosetta@home

So what's the right way then? Look at a very similar grid computing project, [Rosetta@home](!Wikipedia). Rosetta@home has only $\frac{1}{175}$th the computing power of Folding@home and presumably consumes proportionately less electricity; hence it directly kills $\frac{1.01}{175}$ people a year and indirectly kills $\frac{12650}{175} = 72$. This is three orders of magnitude fewer deaths. And with its applied focus, the [benefits](!Wikipedia "Rosetta@home#Disease-related research") are a *little* bit better than Folding@home's.

## Charity is not about helping

When you run a few numbers, this seems like a pretty uncontroversial conclusion. Lots of things are worse charities than the best charities (almost by definition); why be so wedded to the idea that Folding@home is not one of *those* charities? (Why do geeks in particular seem offended by criticism of Folding@home?) I think it has to do with our *real* reasons for a lot of things - social status[^status]. Philanthropy is often for such worthless activities (does the MoMA *really* need donations from its board of directors so it can buy the latest artwork to have been priced into the stratosphere?) that the truth of the matter - a straightforward cash-for-status bargain - is obvious[^newyorker], but it's not so obvious that charities themselves seek status-raising activities and so are biased towards funding bizarre & novel new activities[^givewell] - and what is more bizarre & novel than building a worldwide supercomputer to calculate the folding of proteins?

[^status]: Status as I use it here is a bit complex, more than a little idiosyncratic, and as much a paradigm as any simple property. To get an idea of what I mean, see [Wikipedia](!Wikipedia "Social status"), [LessWrong](http://wiki.lesswrong.com/wiki/Status#Blog_posts)/[Overcoming](http://www.overcomingbias.com/tag/status)[Bias](http://www.overcomingbias.com/tag/signaling), and [Tyler Cowen](!Wikipedia)'s description of [Robin Hanson](!Wikipedia) in [_Discover your Inner Economist_](http://hanson.gmu.edu/press/DiscoverYourInnerEconomist-excerpt.txt).
[^newyorker]: _The New Yorker_, with its focus on New York City's upper-crust, recently made this clear to me yet again with its coverage of the [Koch brothers](!Wikipedia), who would more usually be greeted by said upper-crust with derision than acclaim; from 2010's ["Covert Operations: The billionaire brothers who are waging a war against Obama"](http://www.newyorker.com/reporting/2010/08/30/100830fa_fact_mayer?currentPage=all):

    > "On May 17th, a black-tie audience at the Metropolitan Opera House applauded as a tall, jovial-looking billionaire took the stage. It was the seventieth annual spring gala of American Ballet Theatre, and David H. Koch was being celebrated for his generosity as a member of the board of trustees; he had recently donated $2.5 million toward the company's upcoming season, and had given many millions before that. Koch received an award while flanked by two of the gala's co-chairs, Blaine Trump, in a peach-colored gown, and Caroline Kennedy Schlossberg, in emerald green. Kennedy's mother, Jacqueline Kennedy Onassis, had been a patron of the ballet and, coincidentally, the previous owner of a Fifth Avenue apartment that Koch had bought, in 1995, and then sold, eleven years later, for thirty-two million dollars, having found it too small.
    >
    > The gala marked the social ascent of Koch, who, at the age of seventy, has become one of the city's most prominent philanthropists. In 2008, he donated a hundred million dollars to modernize Lincoln Center's New York State Theatre building, which now bears his name. He has given twenty million to the American Museum of Natural History, whose dinosaur wing is named for him. This spring, after noticing the decrepit state of the fountains outside the Metropolitan Museum of Art, Koch pledged at least ten million dollars for their renovation. He is a trustee of the museum, perhaps the most coveted social prize in the city, and serves on the board of Memorial Sloan-Kettering Cancer Center, where, after he donated more than forty million dollars, an endowed chair and a research center were named for him.
    >
    > One dignitary was conspicuously absent from the gala: the event's third honorary co-chair, Michelle Obama. Her office said that a scheduling conflict had prevented her from attending. Yet had the First Lady shared the stage with Koch it might have created an awkward tableau..."

    A quick calculation: $3+100+20+10+40=173$, so a vastly incomplete tally of the Koch donations is $200 million or roughly 200,000 dead Africans. Does anyone want to argue that the Kochs' philanthropy is even *remotely* close to being efficient, and that these donations were anything but purchasing status? In some cases, one wonders why they even try to pretend the event was charity; ["More Cash to Go to a Hall Than to Haiti"](http://www.nytimes.com/2010/03/18/arts/music/18benefit.html), _The New York Times_:

    > "Even if the event's nearly $200,000 worth of tickets sell out, less than $8,000 from the sales will go to the cause. The concert, though, is expected to raise some money, thanks mainly to a $50,000 subsidy by the Montblanc company and $10,000 by CAMI Music, the concert's presenter and Mr. Lang's management agency...No hard and fast guidelines exist on how much money raised in a benefit should go for expenses, and it is not unusual for galas to raise little money or even lose it...In an accounting provided by CAMI Music, the costs will total $181,590. If the hall sells out, box office proceeds will total $189,793, excluding complimentary tickets."

[^givewell]: From the [GiveWell blog](http://blog.givewell.org/), ["After “Extraordinary and Unorthodox” comes the Valley of Death"](http://blog.givewell.org/2010/11/17/after-extraordinary-and-unorthodox-comes-the-valley-of-death/) (replacing the relevant adjectives with 'status' is left as an exercise for the reader):

    > "...it's hard for me to see a big difference between it and the $100 million Gates Grand Challenges Explorations, “a unique initiative that supports innovative research of unorthodox ideas” in global health (though the [InnoCentive](!Wikipedia) proposal above does not explicitly specify a sector, all three of its examples are in global health as well).
    >
    > Speaking more informally, I've heard similar concepts emphasized by most major funders I've spoken with. Anyone who has dealt with major foundations should recognize the desire to find a completely new, revolutionary, neglected opportunity that just needs some seed funding to explode.
    >
    > I do believe that the best opportunities are the under-funded ones. Yet I'm not sure that tiny, neglected innovations are the best places to look for these opportunities - precisely because that's where all the major funders seem to be looking. I submit that the better place to look for neglected opportunities is the “valley of death” between proof of concept and large-scale rollout.
    >
    > ...There's no glory in funding the [VillageReach](!Wikipedia) rollout. VillageReach already has shown that what it's doing has worked; nobody can claim to be brilliant for spotting it. And VillageReach doesn't need help designing its program (this has been cited to me explicitly as a drawback from the perspective of some major funders)."

    From ["Profile of a GiveWell Customer"](http://blog.givewell.org/2011/06/02/profile-of-a-givewell-customer/):

    > "Attitudes toward evidence seem less key than we would have guessed. When we started GiveWell, we and most of our supporters imagined that new customers could be found in certain industries where people are accustomed to using measurement to evaluate and learn from their decisions. We hoped these people would resonate with our desire to bring feedback loops into areas where feedback loops don't naturally exist. But we've found that a lot of them don't, largely because impact isn't the main thing they're aiming for when they give. People give for many reasons - to maintain friendships, to overcome guilt and cognitive dissonance, to achieve recognition - and a given donor is unlikely to be interested in GiveWell unless achieving impact is at the top of his/her list."

    On the other hand,

    > "GiveWell customers never seem interested in public recognition. In our first year, we considered posting acknowledgments to major supporters on our website, but there was no interest. Since then, we have had many customers who require anonymity (even when we ask them to take public credit for our sake) and no customers who've requested that we publicly thank them or otherwise help them get recognition."

    It is a little difficult for Hansonian theories of charity to explain wholly anonymous charity. Possibly charity in such cases is due to signaling within a small group, rather than public signaling to thousands or millions of people (the general populace). And who knows; maybe there is genuine altruism in this world of dust.

It is sad and pitiable that we spend so many billions on things like dog food and cosmetics rather than saving lives; but isn't it even sadder that we can avoid that error, and *try* to do good, and *still* fail? The only thing sadder, I think, would be if we could know of our failure and go on supporting Folding@home - if charity was not about helping people.

# A Bitcoin+BitTorrent-driven economy for creators (Artcoin)

    < foucist> sipa: gwern> idea is to convert the hashes BitTorrent does already into Bitcoin style hashes, which are only  assemble-able by the server; if the
               creator=server, then we just invented a way to turn downloads  into nanopayments with zero mental transaction costs

    < gwern> sipa: well, let's call this Artcoin. suppose there were a Artcoin-based economy but there were no upper limit but a built-in steady inflation. if downloads
             solve hashes along the way, we've made a system which gives originators of popular torrents some bitcoins, and the more popular they are, the more bitcoins structured right, this might be the most popular way of downloading - solving the issue of 'BitTorrent/p2p is killing ...
    < gwern> ... creators' livelihoods!'
    < foucist> it might be entirely possible to do that with the current Bitcoin system,  modify BitTorrent to use a different combo hash that ties into bitcoins somehow
    < gwern> sipa: I thought about the Bitcoin increasing difficulty thing, but it seems to me that this would discourage creators. what happens when generating a new
             Bitcoin becomes a rare event, like once a year or something? the incentive for the creator disappears.
    < foucist> gwern: well the argument is that bitcoins isn't about mining
    < gwern> 'even if I release my awesome new movie, I probably won't get this year's Bitcoin. so why bother?'

    < gwern> sipa: paying creators using inflation isn't that bad an idea. inflation is invisible and doesn't require assent. assent is why microtransaction schemes have
             failed in their thousands
    < foucist> i think the base idea is to find a way to tie bitcoins into BitTorrent such that it's an automatic micropayment for creators of that content
    < sipa> but you're describing two ideas
    < gwern> (or at least, hundreds. micropayments seems like the sort of thing Internet folks have been trying for ages and failing miserably every time.)
    < sipa> 1. using bitcoins to pay for downloads
    < sipa> you mean content creators
    < sipa> 2. reusing the hashing done by BitTorrent for Bitcoin
    < gwern> the users mine as a side-effect of downloading for the benefit of the server/tracker, who may be the creator (given the creator's obvious first-mover
             advantage)
    < sipa> furthermore, i don't see how you are going to reuse the hashing power... BitTorrent uses it in a deterministic way to verify whether data is identical, while
            Bitcoin uses it as a pseudorandom function to look for a lot of zeroes
    < gwern> sipa: #1 is boring. it's the same damn failed idea that has been tried hundreds of times. #2 is more interesting. is there any variant which does both?
    < gwern> that's where I fail because I don't know the math
    < gwern> (is there any way to force mining as part of the download? of course the users/downloaders could volunteer some hashing, but then someone will write a client
             which does no hashing because 'it was making my computer slow' and the scheme collapses)

    < gwern> foucist: I don't think that'd work, because suppose you have no bitcoins? you have an incentive to find a plaintext torrent

    < sipa> and that you pay with Bitcoin hashes to get the data
    < gwern> foucist: 'oh, but my backup server doesn't have access to my Bitcoin account'
    < gwern> sipa: hm, that sounds more sensible. what measures could be put in place to prevent colluding clients?
    < sipa> but that's little more than a using-bitcoins-to-pay-for-data system
    < foucist> gwern: OK so you think only a purely generative way would work ?  hmm
    < sipa> and it's very wasteful
    < sipa> since the profitability of CPU mining is already long gone
    < gwern> sipa: in the current Bitcoin system, yeah, not some new Bitcoin system
    < sipa> explain me why people would value your Artcoin's?
    < gwern> foucist: well, you need to keep the Bitcoin+BitTorrent not much more expensive in time than plain BitTorrent. if the new system took 3 times as long to
             download, then people have an incentive to use a parallel system. but if it were only, say, a 20% penalty, then lots of people would use it. look at iTunes
    < foucist> gwern: Bitcoin mining of any kind is still kinda wasteful though, those CPU power could be used for solving real problems etc ;)
    < sipa> indeed, do that as payment in your paying-for-creators BitTorrent system
    < gwern> sipa: suppose creators began using it. it would have pretty much the same advantages as regular Bitcoin, it'd give you access to the new stuff, you'd not feel
             guilty about piracy, that sort of thing. (again, why does anyone download from iTunes?)
    < sipa> pay with folding@home work units or some
    < sipa> gwern: but it wouldn't have limited total supply

    < gwern> sipa: would people suddenly stop being interested in Bitcoin if the guarantee weren't 0% inflation but, say, 0.1% inflation?
    < sipa> some, certainly
    < gwern> (US GDP is 14.6 trillion, so 0.1% inflation would be a lot.)

    < gwern> foucist: wouldn't surprise me if the research has already been done, actually; this is starting to remind me of the 'proof of work' subfield of crypto
    < Necr0s> I need to understand a bit more about crypto.
    < foucist> a quick google on 'micropayment hash' reveals a variety of research papers on micropayments with hash-chains
    < Necr0s> Particularly asymmetric public/private key systems.
    < foucist> "Micro-payment Protocol Based on Multiple Hash Chains" "A Study of Micro-payment Based on One-Way Hash Chain'
    < Necr0s> And their use in signing content.

    < foucist> gwern: http://www.few.vu.nl/~srijith/publications/confs/icccn08.pdf   "Floodgate: A Micropayment Incentivized P2P Content Delivery Network"  it talks about
                        hash-chains a bit and such, so it could be relevant to bitcoins

Alternate blockchains are not an impossible idea. The [Namecoin](https://en.bitcoin.it/wiki/Namecoin) network is up and running with another blockchain, specialized for registering and paying for domain names.

And there's already a quasi-implementation of this: [Bitcoin Plus](http://www.bitcoinplus.com/miner/embeddable). It is a piece of JavaScript that does the SHA-256 mining like the regular GPU miners. The idea is that one includes a link to it on one's website and then all one's website visitors' browsers will be bitcoin mining while they visit. In effect, they are 'paying' for their visit with their computational power. This is more efficient than [parasitic computing](!Wikipedia) (although visitors could simply disable JavaScript and so it is more avoidable than parasitic computing), but from the global view, it's still highly inefficient: JavaScript is not the best language in which to write tight loops and even if browser JavaScript were up to snuff, CPU mining in general is extremely inefficient compared to GPU mining. Bitcoin Plus works because the costs of electricity and computers is externalized to the visitors. Reportedly, CPU mining is no longer able to even pay for the cost of the electricity involved, so Bitcoin Plus would be an example of [negative externalities](!Wikipedia). A good Artcoin scheme should be Pareto-improving.

# Good governance & Girl Scouts

See [Girl Scouts and good governance]().

# Hard problems in utilitarianism

The Nazis believed many sane things, like exercise and the value of nature and [animal welfare](!Wikipedia "Animal welfare in Nazi Germany") and the harmful nature of smoking.

Possible rationalist [exercise](http://lesswrong.com/lw/us/the_ritual/):

1. Read _The Nazi War on Cancer_
2. Assemble modern demographic & mortality data on cancer & obesity.
3. Consider this hypothetical: 'If the Nazis had not attacked Russia and negotiated a peace with Britain, and remained in control of their territories, would the lives saved by the [health benefits](!Wikipedia "Anti-tobacco movement in Nazi Germany") of their policies outweigh the genocides they were committing?'
4. Did you answer yes, or no? Why?
5. As you pondered these questions, was there ever *genuine* doubt in your mind? Why was there or not?

# Who lives longer, men or women?

Do men or women live longer? Everyone knows [women live a few years longer](!Wikipedia "Life expectancy#Sex differences"); if we look at America and Japan (from the 2011 [CIA World Factbook](!Wikipedia)):

1. 75.92 vs 80.93
2. 78.96 vs 85.72

5-7 years additional bulk longevity is definitely in favor of women. But maybe what we are really interested in is whether women have longer *effective* lives: the amount of time which they have available to pursue those goals, whatever they may be, from raising children to pursuing a career. To take the Japanese numbers, women may live 8.6% longer, but if those same women had to spend 2 hours a day (or 1/12th a life, or 8.3%) doing something utterly useless or distasteful, then maybe one would rather trade off that last 0.3%.

But notice how much we had to assume to bring the female numbers down to male: 2 hours a day! That's a lot. I had not realized how much of a lifetime those extra years represented: it was a larger percentage than I had assumed.

The obvious criticism is that social expectations that women appear as attractive as possible will use up a lot of women time. It's hard to estimate this, but men have to maintain their appearance as well; a random guess would be that men spend half an hour and women spend an hour on average, but that only accounts for a fourth of the extra women time. Let's say that this extra half hour covers make-up, menstruation, waiting in female bathroom lines, and so on. (This random guess may understate the impact; the pill aside, menstruation reportedly can be pretty awful.)

Sleep patterns don't entirely account for the extra time either; one guide says ["duration of sleep appears to be slightly longer in females"](http://sleep.health.am/sleep/sleep-and-gender/), and [Zeo, Inc.](!Wikipedia)'s [sleep dataset](http://blog.myzeo.com/whats-your-zq/) indicates a difference of women sleeping 19 minutes more on average. If we round to 20 minutes and add to the half hour for cosmetics, we're still not even half the way.

And then there's considerations like men becoming disabled at a higher rate than women (from the dangerous jobs or manual labor, if for no other reason).

Pregnancy and raising children is a possible way to even things out. The US census [reports a 2000 figure](http://usgovinfo.about.com/library/weekly/aa031601a.htm) that 19% of women 40-44 did not have children. So the overwhelming majority of women will at some point bear the burden of at least 1 pregnancy. So that's 9 months there, and then...?

That's not even 1 year, so a quarter of the time is left over if we assume the pregnancy is a total time-sink but the women involved do not spend any further time on it (but also that the average male expenditure is zero time, which was never true and is increasingly less true as time passes). That leaves a decent advantage for women of ~2 years.

If you wanted to erase the female longevity advantage, you could argue that between many women having multiple children, and many raising kids full-time at the expense of their careers or non-family goals, that represents a good decade of lost productivity, and averaging it out (81% of 10 years) reduces their effective lives by 8.1 years, and then taking into account the sleep and toiletry issues reduces the number by another 2 years, and now women lifetimes are shorter than men lifetime.

So at least as far as this goes, your treatment of childbearing will determine whether the longevity advantage is simply a fair repayment, as it were, for childbearing and rearing, or whether it really is a gift to the distaff side.

# Considerations upon a weekend in July

To learn to build sandcastles on the beach is to learn to live and die as an atheist.

# Poems on the theme of _Genshiken_

See [Genshiken](),

# Poems on the theme of _Rurouni Kenshin_

[For ANN](http://www.animenewsnetwork.com/contest/2011-08-14/rurouni-kenshin-haiku):

    None remain to see
    how under the pouring skies,
    blood runs down the blade

    None remain to see
    now, under the pouring skies,
    blood pooling with rain.

    Friends & lives are rivers
    that endlessly flow into
    the dark sea of death

    Our lives are rivers
    that endlessly flow into
    quiet seas of death

    With the summer sun
    my birthday comes, and it goes;
    and the leftover
    presents' discarded wrappings
    remind me of my own fate.

# Two poems

Papermachine:

    What zeal!
    the wild nights spent burning
    candles,
    running up mountains,
    churning through paper.

Reply:

    With such zeal and joy
    did I burn those wild nights
    in the candle light,
    bounding up paper piles
    and scaling mountains of thought

# Chinese Kremlinology

> "I'm not suggesting that any of the news pieces above are false, I'm more worrying about my own ability to be a good consumer of news. When I read about Wisconsin, for example, I have enough context to know why certain groups would portray a story in a certain way, and what parts of the story they won't be saying. When I'm interested in national (US) news, I know where to go to get multiple main-stream angles, and I know where to go to get fringe analysis. Perhaps these tools don't amount to much, but I have them and I rely on them. But I really know very little about how news from China gets to me, and it is filtered through a lot more layers than when I read about things of local interest." --[Antoine Latter](https://plus.google.com/u/0/107517520217732606032/posts/jpMQ9jC5z9z)

It *is* dangerous to judge a very large and complex country with truly formidable barriers to understanding and internal opacity. As best as I can judge the numbers and facts presented for myself, there are things rotten in Denmark. (The question is whether they are rotten enough.)

But at the same time, we can't afford to buy into the China-as-the-next-threat hype. When I was much younger, I read every book my library had on Japan's economics and politics, and many online essays & articles & op-eds besides. They were profoundly educational, but not just in the way that their authors had intended - because they were all from the 'Japan as No. 1' / 'Rising Sun' (Michael Crichton) period of the bubble '80s, and they were profoundly confident about how Japan would rule the world and quite convincing but even as I read them, Japan's bubble had popped brutally and it continued to stagnate. This dissonance and my own susceptibility was not lost on me. (Another example from that same period of time - reading _Dune_, approving of Paul's actions, then reading Herbert's essays and _Dune Messiah_ and realizing I had been cheering on a mass murderer and had fallen right into the trap - "I am showing you the superhero syndrome and your own participation in it.")

Years later, I came across [Paul Krugman](!Wikipedia)'s ["The Myth of Asia's Miracle"](http://web.archive.org/web/20090302203414/http://web.mit.edu/krugman/www/myth.html), which told me about a *economic* (as opposed to military or geopolitical) parallel to Japan's ascension that I'd never heard of - Soviet Russia! (And it's worth noting that South Korea, despite its extraordinary growth and own mini-narratives, is still 3k or so below Japan's per-capita.)

Ever since I have been curious as to China's fate - much greater than or comparable total wealth to the US? skeptical of the optimistic forecasts, and mindful of my own fallibility. Falling into the narrative once, with Japan, is understandable; fool me twice with Soviet Russia, that's forgivable; fool me three times with China, and I prove myself a fool.

# The hidden Library of the Long Now

[Mike Darwin](!Wikipedia) told [an interesting story](http://chronopause.com/index.php/2011/08/09/fucked/#comment-3171) in August 2011 of a long-term project that is under way or may have been completed:

> "...he publicly posts them [his predictions], time stamps them and does statistics. That's just brilliant, and it is something I started doing privately in March of 2006. Within a year I found out that I was useless at predicting the kinds of events I thought I would be best at – such as, say, developments in the drug treatment of diseases which I knew a lot about. What I turned out to be really good at was anything to do with failure analysis, where there was a lot of both quantitative and qualitative data were available. For reasons I'll mention only elliptically here, I became interested in econometric data, and I also had the opportunity to travel the world specifically for the purpose of doing “failure analysis reports” on various kinds of infrastructure: the health care system in Mexico, food distribution and pricing in North Africa, the viability of the cruise (ship) industry over the period from 2010 to 2010, potential problems with automated, transoceanic container shipping… The template I was given was to collect data from a wide range of sources – some of which no self respecting academic would, or could approach. There were lots of other people in the study doing the same thing, sometimes in parallel.
>
> I got “recruited” because “the designer” of the study had a difficult problem he and his chosen experts could not solve, namely, how to encode vast amounts of information in a substrate that would, verifiably, last tens of millions of years. One of the participants in this working group brought me along as a guest to one of their sessions. There were all kind of proposals, from the L. Ron Hubbard-Scientology one of writing text on stainless steel plates, to nanolithography using gold… The discussion went on for hours and what impressed me was that no one had any real data or any demonstrated experience with (or for) their putative technology. At lunch, I was introduced to “the designer” and his first question was, “What are you here for?” I told him I was there to solve his problem and that, if he liked, I could tell him how to do what he wanted absent any wild new technology or accelerated aging tests. I said one word to him: GLASS. Organisms trapped in [amber](!Wikipedia) are, of course, the demonstrated proof that even a very fragile and perishable substrate can be stabilized and retain the information encoded in it for tens of millions of years, if not longer. Pick a stable glass, protect it properly, and any reasonable information containing substrate will be stable over geological time periods^[Darwin discusses this further in the context of brain preservation in his ["Science Fiction, Double Feature, 2: Part 2"](http://chronopause.com/index.php/2011/08/05/science-fiction-double-feature-2-part-2/); see also my [plastination]() essay.]. There were a lot of pissed off people who didn't get to stay for the expected (and elaborate) evening meal. As it turned out, “the designer” had another passion, and that was that he collected and used people whom he deemed (and ultimately objectively tested) were “brilliant” at failure analysis. Failure analysis can be either prospective or retrospective, but what it consists of someone telling you what's likely to go wrong with whatever it is you are doing, or why things went wrong after they already have."

Darwin enlarges in an email:

> My second concern is pretty well addressed in my last post, ["Fucked."](http://chronopause.com/index.php/2011/08/09/fucked/) The geopolitical situation is atrocious; much worse than the media sense and vastly, vastly worse than most of the politicians sense. At the very top, in a few places, such as B of A, Citicorp and the IMF, there are people shitting themselves morning, noon and night. Before, they were just shitting themselves in the morning. The "study" I allude to in my response was the work of a really bizarrely interesting human being who is richer than Croesus and completely obsessed with information. He has spent tens of millions analyzing the "planetary social, economic & geopolitical situation" for failure. He wanted a timeline to failure and was smart enough to understand he could never get precision. He wanted and I think he got, a "best completed by" date for his project. By now, I would guess that there are massive packets of glass going into very, very deep holes in a number of places...
>
> Let's just say I read the final report of the study group and I think I have every good reason to be in one hell of hurry.

This all is quite interesting. Can one guess who this mysterious figure is? Let's do some _ad hoc_ reasoning in the spirit of [Fermi calculation](!Wikipedia)s!

Let's see, tens of millions just on the preliminary studies rules out millionaires; add in land purchases and fabrication costs and the project would run into hundreds of millions (eg. it cost [Jeff Bezos](!Wikipedia) something like $50m for his [Clock of the Long Now](!Wikipedia) and most of the work had already been done by the Foundation!), so we can rule out multi-millionaires, leaving just billionaire-class wealth.

Private philanthropy is almost non-existent in China^[You might get the opposite impression reading articles like this [_New York Times_ article](https://www.nytimes.com/2009/09/23/business/global/23donate.html), but consider the flip side of large percentage growth in philanthropy - they must be starting off from a small absolute base!], Russia, and India so although they have many billionaires we can rule out those nationalities. [Australian billionaires](!Wikipedia "Category:Australian billionaires") are fairly rare and mostly in business or the extractive industries, so we can probably rule out Australia too. Combined with Darwin being an English monolingual (as far as I can tell), one can restrict the search to America and England, European at the most exotic.

To strike Darwin - a cryonicist - as weird and extremely intelligent, he probably has a high [Openness](!Wikipedia "Openness to experience") personality rating, suggesting he either inherited his money or made it in tech or finance. Being obsessed with information fits the two latter better than the former. He implies starting in 2006 or 2007, and it's unlikely he was brought in on the ground floor or the obsession started only then, so our billionaire's wealth was probably made in the '80s or '90s or early 2000s at the very latest, in the first or second dot-com boom. This describes a relatively small subset of the 400 or so [American billionaires](!Wikipedia "Category:American billionaires").

Without trawling through Wikipedia's categories, the most obvious suspects for a weird extremely intelligent tech billionaire interested in information are Jeff Bezos, [Larry Page](!Wikipedia) & [Sergey Brin](!Wikipedia), [Larry Ellison](!Wikipedia), [Charles Simonyi](!Wikipedia)[^simonyi], [Jay S. Walker](!Wikipedia)[^jay], [Peter Thiel](!Wikipedia), and [Jerry Yang](!Wikipedia). Of those 6, I think I would rank them by plausibility as follows:

[^simonyi]: Charles Simonyi is actually the first person to come to mind when I think about 'weird wealthy American technologist interested in old and long-term information who has already demonstrated philanthropy on a large scale'
[^jay]: Walker was the second, due to his [library](!Wikipedia "Jay S. Walker#The Walker Library of the History of Human Imagination"). Information on his net wealth isn't too easy to come by, but he was solidly a [billionaire in 2000](http://www.forbes.com/finance/lists/54/2000/LIR.jhtml), at least...

1. Jeff Bezos

    Scattering glass capsules of information is an extremely Long Now idea and Bezos has already bought into the Long Now to the tune of [dozens of millions](http://www.10000yearclock.net/). This alone makes him the most plausible candidate, although his plausibility is damaged by the fact that he is a very busy CEO and has been for the last 2 decades and presumably would have difficulties devoting a lot of time to such a project.
2. Peter Thiel

    He has no direct Long Now links I know of, but he fits the described man even better than Bezos in some respects: he is acutely aware of upcoming [existential threats](!Wikipedia) and [anthropic biases](!Wikipedia)[^thiel] and scatters [his philanthropy](!Wikipedia "Peter Thiel#Philanthropy") widely over highly speculative investments ([seasteading](!Wikipedia), [SIAI](!Wikipedia), 20 under 20, the [Methuselah Mouse Prize](!Wikipedia) etc.). An additional point in his favor is he lives in San Francisco, near by Darwin or Long Now figures like [Stewart Brand](!Wikipedia)
3. Charles Simonyi; similar to Jay S. Walker
4. Page & Brin; while I generally get a utopian Singulitarian vibe off them and their projects and they seem to like publicizing their works, Google Books is relatively impressive and I could see them interested in this sort of thing as a 'insurance policy'.
5. Yang; I don't see anything especially *implausible* about him, but nothing in favor either.
6. Jay S. Walker; his Library quite impressed me when I saw it, indicating considerable respect for the past, a respect conducive to such a project. I initially ranked him at #3 based on old information about his fortune being at $6-7 billion in 2000, but [_Time_](http://www.time.com/time/magazine/article/0,9171,57731,00.html) reported that the dot-com crash had reduced his fortune to $0.33 billion.
7. Ellison; like Jobs, his heart is cold, but he does seem to donate[^ellison] and claims to donate large sums quietly, consistent with the story. As someone who made his billions off database rented long-term, hopefully he has an appreciation of information and a longer-term perspective than most techies.

[^thiel]: See Thiel's essay ["The Optimistic Thought Experiment: In the long run, there are no good bets against globalization"](http://www.hoover.org/publications/policy-review/article/5646)
[^ellison]: And I use 'seem' advisedly; it's remarkable how selfish [his donations all appear to be](!Wikipedia "Larry Ellison#Charitable donations").

(I do not include [Steve Jobs](!Wikipedia) although he is famous and matches a few criteria; as far as I can tell, he has essentially never used his wealth for anything but his own good [like](http://www.businessinsider.com/how-steve-jobs-got-sick-2010-04) [buying](http://www.slate.com/id/2281668/) [new](http://www.slate.com/id/2281668/) [organs](http://www.ama-assn.org/amednews/2009/07/27/prsa0727.htm), and comes off in _[iWoz](!Wikipedia)_ as having sociopathic characteristics; an anonymous Job adviser remarked in 2010 ["Steve expresses contempt for everyone—unless he's controlling them."](http://www.newyorker.com/reporting/2010/04/26/100426fa_fact_auletta). I would be shocked if Jobs was the former employer.)

All this said, I am well aware I haven't looked at even a small percentage of American billionaires, and I could be wrong in focusing on techies - finance is equally plausible (look at [James Harris Simons](!Wikipedia)! if he isn't a plausible candidate, no one is), and inherited wealth still common enough to not be ignored. Pondering the imponderables, I'd give a [15% chance](http://predictionbook.com/predictions/3130) that one of those 6 people was the employer, and perhaps [a 9% chance](http://predictionbook.com/predictions/3131) that the employer was either Bezos, Thiel, or Simonyi, with [Bezos being 4%](http://predictionbook.com/predictions/3132), [Thiel ~3%](http://predictionbook.com/predictions/3133) and [Simonyi 2%](http://predictionbook.com/predictions/3134).

And indeed, Darwin said he didn't recognize several of those names, and implied they were all wrong. Well, it would have been fairly surprising if 15% confidence assertions derived through such dubious reasoning *were* right.

# Things I have changed my mind about

> "...Once we have taken on a definite form, we do not lose it until death."^[Chapter 2 of the _[Chuang-tzu](!Wikipedia)_; [Thomas Cleary](!Wikipedia)'s translation in _Vitality, Energy, Spirit: A Taoist Sourcebook_ (1991), ISBN 978-0877735199.]

It is salutary for the soul to keep a list of things one no longer believes, since [such crises](http://lesswrong.com/lw/ur/crisis_of_faith/) so easily pass from memory (there is no feeling of *being* wrong, only of having *been* wrong[^schulz]). One does not need an [elaborate ritual](http://lesswrong.com/lw/us/the_ritual/) (fun as they are to read about) to change one's mind, but the changes must happen. If you are not changing, you are not growing[^leary]; no one has won the belief lottery and has a monopoly on truth[^lottery].

[^schulz]: One of the few good bits of Kathryn Schulz's 2011 book _Being Wrong_ (part 1) is where she does a more readable version of Wittgenstein's observation (_PI_ Pt II, p. 162), "One can mistrust one's own senses, but not one's own belief. If there were a verb meaning "to believe falsely," it would not have any significant first person, present indicative." Her version goes:

    > "But before we can plunge into the experience of being wrong, we must pause to make an important if somewhat perverse point: there *is* no experience of being wrong.
    >
    > There is an experience of *realizing* that we are wrong, of course. In fact, there is a stunning diversity of such experiences. As we'll see in the pages to come, recognizing our mistakes can be shocking, confusing, funny, embarrassing, traumatic, pleasurable, illuminating, and life-altering, sometimes for ill and sometimes for good. But by definition, there can't be any particular feeling associated with simply *being* wrong. Indeed, the whole reason it's possible to be wrong is that, while it is happening, you are oblivious to it. When you are simply going about your business in a state you will later decide was delusional, you have no idea of it whatsoever. You are like the coyote in the [_Road Runner_](!Wikipedia "Wile E. Coyote and Road Runner") cartoons, after he has gone off the cliff but before he has looked down. Literally in his case and figuratively in yours, you are already in trouble when you feel like you're still on solid ground. So I should revise myself: it does feel like something to be wrong. It feels like being right."
[^leary]: "You're only as young as the last time you changed your mind." --[Timothy Leary](!Wikipedia) (quoted in _Office Yoga: Simple Stretches for Busy People_ (2000) by Darrin Zeer, p. 52)
[^lottery]: "Everyone thinks they've won the Magical Belief Lottery. Everyone thinks they more or less have a handle on things, that they, as opposed to the billions who disagree with them, have somehow _lucked_ into the one true belief system." --[R. Scott Bakker](!Wikipedia), _Neuropath_

This list is not for specific facts of which there are too many to record, nor is it for falsified predictions like my belief that George W. Bush would not be elected (for those see [Prediction markets]() or my [PredictionBook.com page](http://predictionbook.com/users/gwern)), nor mistakes in my personal life, nor things I never had an initial strong position on (Windows vs Linux, Java vs Haskell, theism vs atheism). The following are some major ideas or sets of ideas that I have changed my mind about:

1. the American Revolution

    In middle school, we were assigned a pro-con debate about the [American Revolution](!Wikipedia); I happened to be on the pro side, but as I read through the arguments, I became increasingly disturbed and eventually realized that the pro-Revolution arguments were weak or fallacious. In the long run, independence may have been good for the USA, but this would be due to sheer accident: the British were holding the frontier at the Appalachians (see [Royal Proclamation of 1763](!Wikipedia)), and Napoleon likely would not have been willing engage in the [Louisiana Purchase](!Wikipedia) with English colonies inasmuch as he was at war with England.

    Neither of these is a very strong argument; the British could easily have revoked the Proclamation in face of the _de facto_ resistance, and Napoleon could not hold onto New France for very long against the British fleets. The argument from 'freedom' is a buzzword or unsupported by the facts - Canada and Australia are hardly bastions of totalitarianism, ranked in 2011 by [Freedom House](!Wikipedia) as being as free as the USA. And there is an important counter-argument - Britain ended slavery very early on and likely would have ended slavery in the colonies as well. The South crucially depended on England's tacit support, so the [American Civil War](!Wikipedia) would either never have started or have been suppressed very quickly. The Civil War would also have lacked its intellectual justification of [states' rights](!Wikipedia) if the states had remained Crown colonies. The Civil War was so bloody and destructive^[From Wikipedia: 'It remains the deadliest war in American history, resulting in the deaths of 620,000 soldiers and an undetermined number of civilian casualties. According to John Huddleston, "Ten percent of all Northern males 20–45 years of age died, as did 30 percent of all Southern white males aged 18–40."'] that avoiding it is worth a great deal indeed. And then there comes WWI and WWII. It is not hard to see how America remaining a colony would have been better for both Europe and America.

    Since that paradigm shift in middle school, my view has changed little. Crane Brinton's _[The Anatomy of Revolution](!Wikipedia)_ confirmed my beliefs with statistics about the economic class of participants (naked financial self-interest is not a very convincing argument for plunging a country into war). Mencius Moldbug discussed good deal of [primary source](http://unqualified-reservations.blogspot.com/2009/01/gentle-introduction-to-unqualified_15.html) [material](http://unqualified-reservations.blogspot.com/2007/12/why-i-am-not-libertarian.html) which supported my interpretation. I particularly enjoyed [his description ](http://unqualified-reservations.blogspot.com/2007/12/why-i-am-not-libertarian.html)of the Pulitzer-winning _[The Ideological Origins of the American Revolution](!Wikipedia)_, a study of the popular circulars and essays (of which Thomas Paine's _[Common Sense](!Wikipedia "Common Sense (pamphlet)")_ is only the most famous) finding that the rebels and their leaders believed there was a conspiracy by English elites to strip them of their freedoms and crush the varied Protestants under the yoke of the [Church of England](!Wikipedia), where he points out that no traces of any such conspiracy has ever been found in the diaries or memorandums or letters of said elites and that the Founding Fathers were *exactly* analogous to [9/11 Truthers](!Wikipedia). (He further points out that reality has directly contradicted their beliefs, as the Monarchy and Church of England have seen their power continuously and monotonically decreasing to their present-day ceremonial status, a diminution in progress long before the American Revolution.) Possibly on Moldbug's advice, I then read volume 1 of Murray Rothbard's _[Conceived in Liberty](!Wikipedia)_. I was unimpressed; Rothbard seems to think he is justifying the Revolution as a noble libertarian thing (except for those other scoundrels who just want to take over), but all I saw were scoundrels.
2. Communism

    In roughly middle school as well, I was very interested in economic injustice and guerrilla warfare, which naturally led me straight into the communist literature. I grew out of this when I realized that while I might not be able to pinpoint the problems in communism, a lot of that was due to the sheer obscurity and bullshitting in the literature (I finally gave up with [_Empire_](!Wikipedia "Empire (book)"), concluding the problem was not me, Marxism was really that intellectually worthless), and the practical results spoke for themselves...
3. The Occult

    This is not a particular error but a whole class of them. I was sure that the overall theistic explanations were false, but surely there were real phenomenon going on? I'd read up on individual things like Nostradamus's prophecies or the Lance of Longinus, check the skeptics literature, and disbelieve; rinse and repeat until I finally dismiss the entire area with some exceptions like the mental & physical benefits of meditation. (I am still annoyed that I was unable to disbelieve the research on [Transcendental Meditation](!Wikipedia) until I read more about the corruption and deception and falsified predictions in the TM organization itself.) Fortunately, I had basically given up on occult things by high school, before I found Eco's _[Foucault's Pendulum](!Wikipedia)_, so I don't feel *too* chagrined about this.
4. fiction

    I spend most of my time reading; I also spent most of my time in elementary, middle, and high school reading. What has changed in *what* I read - I now read principally nonfiction (philosophy, economics, random sciences, etc.), where I used to read almost exclusively fiction. (I would include one nonfiction book in my stacks of books to check out, on a sort of 'vegetables' approach. Eat your vegetables and you can have dessert.) I, in fact, aspired to be a novelist. I thought fiction was a noble task, the highest production of humanity, and writers some of the best people around, producing immortal works of truth. Slowly this changed. I realized fiction changed nothing, and when it did change things, it was as oft as not for the worse. Fiction promoted simplification, focus on sympathetic examples, and I recognized how much of my own infatuation with the Occult (among other errors) could be traced to fiction. What a strange belief, that you could find truths in lies. And there are so many of them, too! So very many. (I wrote one essay on this topic, [Culture is not about Esthetics]().) I still produce [some fiction](index#fiction) these days, but mostly when I can't help it or as a writing exercise.
5. [Nicotine]()

    I changed my mind about nicotine in 2011. I had naturally assumed, in line with the usual American cultural messages, that there was nothing good about tobacco and that smoking is deeply shameful, proving that you are a selfish lazy short-sighted person who is happy to commit slow suicide (taking others with him via second-hand smoke) and cost society a fortune in medical care. Then some mentions of nicotine as useful came up and I began researching it. I'm still not a fan of *smoking*, and I regard any tobacco with deep trepidation, but [the research literature](Nicotine#performance) seems pretty clear: nicotine enhances mental performance in multiple domains and may have some minor health benefits to boot. Nicotine sans tobacco seems like a clear win. (It amuses me that of the changes listed here, this is probably the one people will find most revolting and bizarre.)

There are some things I used to be certain about, but I am no longer certain either way; I await future developments which may tip me one way or the other.

1. [the Singularity](!Wikipedia) is near

    In the 1990s, all the numbers seem to be ever-accelerating. Indeed, I could feel with Kurzweil that _[The Singularity is Near](!Wikipedia)_. But an odd thing happened in the 2000s (a dreary decade, distracted by the dual dissipation of Afghanistan & Iraq). The hardware kept getting better mostly in line with Moore's Law (troubling as the flight to parallelism is), but the AI software didn't seem to keep up. I am only a layman, but it looks as if all the AI applications one might cite in 2011 as progress are just old algorithms now practical with newer hardware. And economic growth slowed down, and the stock market ticked along, barely maintaining itself. The Human Genome Project completely fizzled out, with interesting insights and not much else. (It's great that genome sequencing has improved exactly as promised, but what about *everything else*? Where are our embryo selections, our germ-line engineering, our universal genetic therapies, our customized drugs?) The pharmaceutical industry has reached such diminishing returns that even the optimists have noticed the problems in the drug pipeline, problems so severe that it's hard to wave them away as due to that dratted FDA or ignorant consumers. Kurzweil published [an evaluation of his predictions](http://www.kurzweilai.net/predictions.php) up to ~2009 with great fanfare and self-congratulation, but reading through them, I was struck by how many he weaseled out on (claiming as a hit anything that existed in a lab or a microscopic market segment, even though in context he had clearly expected it to be widespread) and how often they failed due to unintelligent software.

    And there are many troubling long-term metrics. I was deeply troubled to read [Charles Murray](!Wikipedia)'s _[Human Accomplishment](!Wikipedia)_ pointing out a long-term decline in discoveries per capita (despite ever increasing scientists and artists per capita!), even after he corrected for everything he could think of. I didn't see any obvious mistakes. [Tyler Cowen](!Wikipedia)'s __[The Great Stagnation](!Wikipedia)_ twisted the knife further, and then I read [Joseph Tainter](!Wikipedia)'s _[The Collapse of Complex Societies](!Wikipedia)_. I have kept notes since and see little reason to expect a general exponential upwards. The Singularity is still more likely than not, but these days, I tend to look towards emulation of human brains as the cause. Whole brain emulation is not likely for many decades, given the extreme computational demands (even if we are optimistic and take the Whole Brain Emulation Roadmap figures, one would not expect a upload until the 2030s) and it's not clear how useful an upload would be in the first place. It seems entirely possible that the mind will run slowly, be able to self-modify only in trivial ways, and in general be a curiosity akin to the Space Shuttle than a pivotal moment in human history deserving of the title Singularity.

    The difficulty is that a 'pure' AI is perfectly possible, and if the AI is not run at the exact instant that there is enough processing power available, ever more computing power in excess of what is needed (by definition) builds up. It is like a dry forest roasting in the summer sun: the longer the wait until the match, the faster and hotter the wildfire will burn[^overhang]. Perhaps paradoxically, the longer I live without seeing an AI of any kind, the wider my forecasts become - I will predict with increasingly high confidence normality (because the non-appearance makes it increasingly likely AI will not appear in the next time-unit and also increasingly likely AI is not possible, see [the hope function & AI](http://lesswrong.com/lw/5hq/an_inflection_point_for_probability_estimates_of/428t)), but (in the increasingly improbable event of AI) the changes I predict become ever more radical.
2. racial IQ differences

    This one may be even more inflammatory than supporting nicotine, but it's an important entry on any honest list. I never doubted that IQ was in part hereditary (Stephen Jay Gould aside, this is too obvious - what, everything from drug responses to skin and eye color would be heritable *except* the most important things which would have a huge effect on reproductive fitness?), but all the experts seemed to say that diluted over entire populations, any tendency would be non-existent. Well, OK, I could believe that; visible traits consistent over entire populations like skin color might differ systematically because of sexual selection or something, but why not leave IQ following the exact same bell curve in each population? There was no specific thing here that made me start to wonder, more a gradual undermining (Gould's _[The Mismeasure of Man](!Wikipedia)_ being completely dishonest is one example - with enemies like that...) and lack of really convincing counter-evidence like one would expect the last two decades to have produced given the politics involved.
3. [Neo-Luddism](!Wikipedia "Luddite#In contemporary thought")

    The idea of permanent [structural unemployment](!Wikipedia) and a [jobless recovery](!Wikipedia) used to be dismissed contemptuously as the [Luddite fallacy](!Wikipedia). (There are models where technology *does* produce permanent unemployment, and quite plausible ones too; see [Autor et al](http://econ-www.mit.edu/files/569)[^hansongrow] and Krugman's [commentary](http://krugman.blogs.nytimes.com/2011/03/06/autor-autor/) pointing to [recent data](http://www.nber.org/papers/w16082) showing the 'hollowing out' and 'deskilling' predicted by the Autor model. Martin Ford has [some graphs](http://www.asymptosis.com/are-machines-replacing-humans-or-am-i-a-luddite.html) explaining the complementation-substitution model.) But ever since the Internet bubble burst, it's been looking more and more likely, with scads of evidence for it since the housing bubble. (This is closely related to my grounds for believing in a distant Singularity.) When I look around, it seems to me that we have been suffering tremendous unemployment for a long time. When Alex Tabarrok writes "If the Luddite fallacy were true we would all be out of work because productivity has been increasing for two centuries", I think, isn't that correct? If you're not a student, you're retired; if you're not retired, you're disabled; if you're not disabled, perhaps you are institutionalized; if you're not that, maybe you're on welfare, or just unemployed. Compare now to the 1300s - every kid in special ed would be out working on the farm, everyone in college would be out working (because the number of students was a rounding error and they didn't spend very long in higher education to begin with), retirees didn't exist outside the tiny nobility, and so on. In contrast, Americans now spend most of their lives not working as they are some non-working status. The unemployment rate looks good - 9% is surely a refutation of the Luddite fallacy! - until you look into the meat factory and see that that is the best rate, for college graduates actively looking for jobs, and not the overall population one including those who have given up. Economist [Alan Krueger](http://www.bloomberg.com/news/2011-03-31/why-unemployment-rose-so-much-dropped-so-fast-commentary-by-alan-krueger.html) writes of the ratio (which covers *only* 15-64 year olds):

    > "Tellingly, the [employment-to-population rate](!Wikipedia "Employment-to-population ratio") has hardly budged since reaching a low of 58.2 percent in December 2009. Last month it stood at just 58.4 percent. Even in the expansion from 2002 to 2007 the share of the population employed never reached the peak of 64.7 percent it attained before the March-November 2001 recession."

    What do you suppose the rate was in 1300 in the poorer 99% of the world population (remembering how homemaking and raising children is effectively a full-time job)? I'd bet it was a lot higher than the world record in 2005, Iceland's 84%. And Iceland is a very brainy place. What are the merely average with IQs of 100-110 supposed to do? (Heck, what is the half of America with IQs in that region or below supposed to do? Learn C++ and statistics so they can work on Wall Street?) If you want to see the future, look at our youth; where are [summer jobs](http://www.businessweek.com/print/magazine/content/07_52/b4064058743638.htm) these days? Gregory Clark comments sardonically (although he was likely not thinking of [whole brain emulation](http://hanson.gmu.edu/uploads.html)) in _[Farewell to Alms](!Wikipedia)_:

    > "Thus, while in preindustrial agrarian societies half or more of the national income typically went to the owners of land and capital, in modern industrialized societies their share is normally less than a quarter. Technological advance might have been expected to dramatically reduce unskilled wages. After all, there was a class of workers in the preindustrial economy who, offering only brute strength, were quickly swept aside by machinery. By 1914 most horses had disappeared from the British economy, swept aside by steam and internal combustion engines, even though a million had been at work in the early nineteenth century. When their value in production fell below their maintenance costs they were condemned to the knacker's yard."

    Technology may increase total wealth under many models, but there's a key loophole in the idea of 'Pareto-improving' gains - *they don't **ever** have to make some people better off*. And a Pareto-improvement is a good result! Many models don't guarantee even that - it's perfectly possible to become worse off (see the horses above and the fate of humans in Hanson's 'crack of a future dawn' scenario). This is closely related to the '"Luddite fallacy" fallacy'^[Along the lines of the [Pascal's Wager Fallacy Fallacy](http://lesswrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/).], as I've named this: technologists who are extremely intelligent and have worked most of their life with the sort of people who qualify for [MENSA](!Wikipedia) confidently say that 'if there is structural unemployment (and I'm being generous in granting you Luddites even this contention), well, better education and training will fix that!' It's a little hard to appreciate what a stupendous mixture of availability bias, infinite optimism, and plain denial of intelligence differences this all is. [Marc Andreessen](http://online.wsj.com/article/SB10001424053111903480904576512250915629460.html) offers an example in 2011:

    > "Secondly, many people in the U.S. and around the world lack the education and skills required to participate in the great new companies coming out of the software revolution. This is a tragedy since every company I work with is absolutely starved for talent. Qualified software engineers, managers, marketers and salespeople in Silicon Valley can rack up dozens of high-paying, high-upside job offers any time they want, while national unemployment and underemployment is sky high. This problem is even worse than it looks because many workers in existing industries will be stranded on the wrong side of software-based disruption and may never be able to work in their fields again. There's no way through this problem other than education, and we have a long way to go."

    I see. So all we have to do with all the people with <120 IQs, who struggled with algebra and never made it to calculus (when they had the self-discipline to learn it at all), is just to train them into world-class software engineers and managers; and we have to do this for the first time in history. Well, why didn't you say so! We'll get on that right away! Most economists continues to dismiss this line of thought, saying things will work themselves out somehow. Robin Hanson, for example, [seems to think that](http://www.overcomingbias.com/2009/10/take-both-econ-tech-seriously.html) and he's a better economist than me and has thought a great deal about AI and [the economic implications](http://hanson.gmu.edu/aigrow.pdf). Their opposition to Neo-Luddism is about the only reason I remain uncertain.

[^hansongrow]: Or Robin Hanson's paper, ["Economic Growth Given Machine Intelligence"](http://hanson.gmu.edu/aigrow.pdf)

    > "Machines complement human labor when they become more productive at the jobs they perform, but machines also substitute for human labor by taking over human jobs. At first, expensive hardware and software does only the few jobs where computers have the strongest advantage over humans. Eventually, computers do most jobs. At first, complementary effects dominate, and human wages rise with computer productivity. But eventually substitution can dominate, making wages fall as fast as computer prices now do. An intelligence population explosion makes per-intelligence consumption fall this fast, while economic growth rates rise by an order of magnitude or more."
[^overhang]: This is known as the 'overhang' argument. The development and canonical form of it is unclear; it may simply be Singulitarian folklore-knowledge. Eliezer Yudkowsky, from the 2008 ["Hard Takeoff"](http://lesswrong.com/lw/wf/hard_takeoff/):

    > "Or consider the notion of sudden resource bonanzas. Suppose there's a semi-sophisticated Artificial General Intelligence running on a cluster of a thousand CPUs. The AI has not hit a wall - it's still improving itself - but its self-improvement is going so *slowly* that, the AI calculates, it will take another fifty years for it to engineer / implement / refine just the changes it currently has in mind. Even if this AI would go FOOM eventually, its current progress is so slow as to constitute being flatlined...
    >
    > So the AI turns its attention to examining certain blobs of binary code - code composing operating systems, or routers, or DNS services - and then takes over all the poorly defended computers on the Internet. This may not require what humans would regard as genius, just the ability to examine lots of machine code and do relatively low-grade reasoning on millions of bytes of it. (I have a saying/hypothesis that a *human* trying to write *code* is like someone without a visual cortex trying to paint a picture - we can do it eventually, but we have to go pixel by pixel because we lack a sensory modality for that medium; it's not our native environment.) The Future may also have more legal ways to obtain large amounts of computing power quickly.
    >
    > ...A subtler sort of hardware overhang, I suspect, is represented by modern CPUs have a 2GHz *serial* speed, in contrast to neurons that spike 100 times per second on a good day.  The "hundred-step rule" in computational neuroscience is a rule of thumb that any postulated neural algorithm which runs in realtime has to perform its job in less than 100 *serial* steps one after the other.  We do not understand how to efficiently use the computer hardware we have now, to do intelligent thinking.  But the much-vaunted "massive parallelism" of the human brain, is, I suspect, [mostly cache lookups](http://lesswrong.com/lw/k5/cached_thoughts/) to make up for the sheer awkwardness of the brain's *serial* slowness - if your computer ran at 200Hz, you'd have to resort to all sorts of absurdly massive parallelism to get anything done in realtime.  I suspect that, if *correctly designed*, a midsize computer cluster would be able to get high-grade thinking done at a serial speed much faster than human, even if the total parallel computing power was less.
    >
    > So that's another kind of overhang: because our computing hardware has run so far ahead of AI *theory*, we have incredibly fast computers we don't know how to use *for thinking*; getting AI *right* could produce a huge, discontinuous jolt, as the speed of high-grade thought on this planet suddenly dropped into computer time.
    >
    > A still subtler kind of overhang would be represented by human [failure to use our gathered experimental data efficiently](http://lesswrong.com/lw/qk/that_alien_message/)^[A better link on inefficient human induction is probably Phil Goetz's ["Information Theory and FOOM"](http://lesswrong.com/lw/1bm/the_efficaciousness_of_information/).]."

    [Anders Sandberg & Carl Shulman](http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html) gave a 2010 talk on it; from the blog post:

    > We give an argument for why - if the AI singularity happens - an early singularity is likely to be slower and more predictable than a late-occurring one....
    >
    > If you are on the hardware side, how much hardware do you believe will be available when the first human level AI occurs? You should expect the first AI to be pretty close to the limits of what researchers can afford: a project running on the future counterpart to Sequoia or the Google servers. There will not be much extra computing power available to run more copies. An intelligence explosion will be bounded by the growth of more hardware.
    >
    > If you are on the software side, you should expect that hardware has continued to increase after passing "human equivalence". When the AI is finally constructed after all the human and conceptual bottlenecks have passed, hardware will be much better than needed to just run a human-level AI. You have a "hardware overhang" allowing you to run many copies (or fast or big versions) immediately afterwards. A rapid and sharp intelligence explosion is possible.
    >
    > This leads to our conclusion: if you are an optimist about software, you should expect an early singularity that involves an intelligence explosion that at the start grows "just" as Moore's law (or its successor). If you are a pessimist about software, you should expect a late singularity that is very sharp. It looks like it is hard to coherently argue for a late but smooth singularity.
    >
    > ...Note that sharp, unpredictable singularities are dangerous. If the breakthrough is simply a matter of the right insights and experiments to finally cohere (after endless disappointing performance over a long time) and then will lead to an intelligence explosion nearly instantly, then most societies will be unprepared, there will be little time to make the AIs docile, there are strong first-mover advantages and incentives to compromise on safety. A recipe for some nasty dynamics."

    [Jaan Tallinn](http://hplusmagazine.com/2011/07/08/future-technology-merger-or-trainwreck/) in 2011:

    > "It's important to note that with every year the AI algorithm remains unsolved, the hardware marches to the beat of Moore's Law – creating a massive hardware overhang. The first AI is likely to find itself running on a computer that's several orders of magnitude faster than needed for human level intelligence. Not to mention that it will find an Internet worth of computers to take over and retool for its purpose."
